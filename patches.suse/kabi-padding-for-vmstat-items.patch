From: Vlastimil Babka <vbabka@suse.cz>
Subject: kabi padding for vmstat items
Patch-mainline: Never, kabi placeholder
References: bsc#1220507

Adding new zone/node accounting fields is difficult without breaking kabi so
let's add some placeholder values to all the related enums.

The printing code is adjusted to skip them either by using a lower max index or
relying on the human readable names in vmstat_text pointing to a specific string
vmstat_text_unused with the contents "unused_counter".

This should be robust so in case any 3rd party module code or future backport
uses the full max index, it will just produce extra output with
"unused_counter" and zero value.

Where possible, counteris folding and other manipulation code is also adjusted
to not handle the unused entries.

Signed-off-by: Vlastimil Babka <vbabka@suse.cz>

---
 drivers/base/node.c    |    6 ++--
 include/linux/mmzone.h |   19 +++++++++++++++
 mm/memcontrol.c        |    2 -
 mm/vmstat.c            |   62 ++++++++++++++++++++++++++++++-------------------
 4 files changed, 61 insertions(+), 28 deletions(-)

--- a/drivers/base/node.c
+++ b/drivers/base/node.c
@@ -521,20 +521,20 @@ static ssize_t node_read_vmstat(struct d
 	int i;
 	int len = 0;
 
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++)
 		len += sysfs_emit_at(buf, len, "%s %lu\n",
 				     zone_stat_name(i),
 				     sum_zone_node_page_state(nid, i));
 
 #ifdef CONFIG_NUMA
 	fold_vm_numa_events();
-	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)
+	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++)
 		len += sysfs_emit_at(buf, len, "%s %lu\n",
 				     numa_stat_name(i),
 				     sum_zone_numa_event_state(nid, i));
 
 #endif
-	for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 		unsigned long pages = node_page_state_pages(pgdat, i);
 
 		if (vmstat_item_print_in_thp(i))
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -120,10 +120,16 @@ enum numa_stat_item {
 	NUMA_INTERLEAVE_HIT,	/* interleaver preferred this zone */
 	NUMA_LOCAL,		/* allocation from local node */
 	NUMA_OTHER,		/* allocation from other node */
+	SUSE_KABI_NUMA_STAT_PADDING1,
+#ifndef __GENKSYMS__
+	NR_VM_NUMA_EVENT_ITEMS_USED = SUSE_KABI_NUMA_STAT_PADDING1,
+#endif
+	SUSE_KABI_NUMA_STAT_PADDING2,
 	NR_VM_NUMA_EVENT_ITEMS
 };
 #else
 #define NR_VM_NUMA_EVENT_ITEMS 0
+#define NR_VM_NUMA_EVENT_ITEMS_USED 0
 #endif
 
 enum zone_stat_item {
@@ -146,6 +152,11 @@ enum zone_stat_item {
 #ifdef CONFIG_UNACCEPTED_MEMORY
 	NR_UNACCEPTED,
 #endif
+	SUSE_KABI_ZONE_STAT_PADDING1,
+#ifndef __GENKSYMS__
+	NR_VM_ZONE_STAT_ITEMS_USED = SUSE_KABI_ZONE_STAT_PADDING1,
+#endif
+	SUSE_KABI_ZONE_STAT_PADDING2,
 	NR_VM_ZONE_STAT_ITEMS };
 
 enum node_stat_item {
@@ -204,6 +215,14 @@ enum node_stat_item {
 	PGPROMOTE_SUCCESS,	/* promote successfully */
 	PGPROMOTE_CANDIDATE,	/* candidate pages to promote */
 #endif
+	SUSE_KABI_NODE_STAT_PADDING1,
+#ifndef __GENKSYMS__
+	NR_VM_NODE_STAT_ITEMS_USED = SUSE_KABI_NODE_STAT_PADDING1,
+#endif
+	SUSE_KABI_NODE_STAT_PADDING2,
+	SUSE_KABI_NODE_STAT_PADDING3,
+	SUSE_KABI_NODE_STAT_PADDING4,
+	SUSE_KABI_NODE_STAT_PADDING5,
 	NR_VM_NODE_STAT_ITEMS
 };
 
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -5645,7 +5645,7 @@ static void mem_cgroup_css_rstat_flush(s
 
 		lstatc = per_cpu_ptr(pn->lruvec_stats_percpu, cpu);
 
-		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 			delta = pn->lruvec_stats.state_pending[i];
 			if (delta)
 				pn->lruvec_stats.state_pending[i] = 0;
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -169,7 +169,7 @@ EXPORT_SYMBOL(vm_node_stat);
 #ifdef CONFIG_NUMA
 static void fold_vm_zone_numa_events(struct zone *zone)
 {
-	unsigned long zone_numa_events[NR_VM_NUMA_EVENT_ITEMS] = { 0, };
+	unsigned long zone_numa_events[NR_VM_NUMA_EVENT_ITEMS_USED] = { 0, };
 	int cpu;
 	enum numa_stat_item item;
 
@@ -177,11 +177,11 @@ static void fold_vm_zone_numa_events(str
 		struct per_cpu_zonestat *pzstats;
 
 		pzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);
-		for (item = 0; item < NR_VM_NUMA_EVENT_ITEMS; item++)
+		for (item = 0; item < NR_VM_NUMA_EVENT_ITEMS_USED; item++)
 			zone_numa_events[item] += xchg(&pzstats->vm_numa_event[item], 0);
 	}
 
-	for (item = 0; item < NR_VM_NUMA_EVENT_ITEMS; item++)
+	for (item = 0; item < NR_VM_NUMA_EVENT_ITEMS_USED; item++)
 		zone_numa_event_add(zone_numa_events[item], zone, item);
 }
 
@@ -774,13 +774,13 @@ static int fold_diff(int *zone_diff, int
 	int i;
 	int changes = 0;
 
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++)
 		if (zone_diff[i]) {
 			atomic_long_add(zone_diff[i], &vm_zone_stat[i]);
 			changes++;
 	}
 
-	for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++)
 		if (node_diff[i]) {
 			atomic_long_add(node_diff[i], &vm_node_stat[i]);
 			changes++;
@@ -809,8 +809,8 @@ static int refresh_cpu_vm_stats(bool do_
 	struct pglist_data *pgdat;
 	struct zone *zone;
 	int i;
-	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };
-	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };
+	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS_USED] = { 0, };
+	int global_node_diff[NR_VM_NODE_STAT_ITEMS_USED] = { 0, };
 	int changes = 0;
 
 	for_each_populated_zone(zone) {
@@ -819,7 +819,7 @@ static int refresh_cpu_vm_stats(bool do_
 		struct per_cpu_pages __percpu *pcp = zone->per_cpu_pageset;
 #endif
 
-		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++) {
 			int v;
 
 			v = this_cpu_xchg(pzstats->vm_stat_diff[i], 0);
@@ -870,7 +870,7 @@ static int refresh_cpu_vm_stats(bool do_
 	for_each_online_pgdat(pgdat) {
 		struct per_cpu_nodestat __percpu *p = pgdat->per_cpu_nodestats;
 
-		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 			int v;
 
 			v = this_cpu_xchg(p->vm_node_stat_diff[i], 0);
@@ -895,15 +895,15 @@ void cpu_vm_stats_fold(int cpu)
 	struct pglist_data *pgdat;
 	struct zone *zone;
 	int i;
-	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };
-	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };
+	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS_USED] = { 0, };
+	int global_node_diff[NR_VM_NODE_STAT_ITEMS_USED] = { 0, };
 
 	for_each_populated_zone(zone) {
 		struct per_cpu_zonestat *pzstats;
 
 		pzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);
 
-		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++) {
 			if (pzstats->vm_stat_diff[i]) {
 				int v;
 
@@ -914,7 +914,7 @@ void cpu_vm_stats_fold(int cpu)
 			}
 		}
 #ifdef CONFIG_NUMA
-		for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++) {
 			if (pzstats->vm_numa_event[i]) {
 				unsigned long v;
 
@@ -931,7 +931,7 @@ void cpu_vm_stats_fold(int cpu)
 
 		p = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu);
 
-		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++)
+		for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++)
 			if (p->vm_node_stat_diff[i]) {
 				int v;
 
@@ -954,7 +954,7 @@ void drain_zonestat(struct zone *zone, s
 	unsigned long v;
 	int i;
 
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++) {
 		if (pzstats->vm_stat_diff[i]) {
 			v = pzstats->vm_stat_diff[i];
 			pzstats->vm_stat_diff[i] = 0;
@@ -963,7 +963,7 @@ void drain_zonestat(struct zone *zone, s
 	}
 
 #ifdef CONFIG_NUMA
-	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++) {
 		if (pzstats->vm_numa_event[i]) {
 			v = pzstats->vm_numa_event[i];
 			pzstats->vm_numa_event[i] = 0;
@@ -1166,6 +1166,8 @@ int fragmentation_index(struct zone *zon
 					TEXT_FOR_HIGHMEM(xx) xx "_movable", \
 					TEXT_FOR_DEVICE(xx)
 
+static const char vmstat_text_unused[] = "unused_counter";
+
 const char * const vmstat_text[] = {
 	/* enum zone_stat_item counters */
 	"nr_free_pages",
@@ -1184,6 +1186,8 @@ const char * const vmstat_text[] = {
 #ifdef CONFIG_UNACCEPTED_MEMORY
 	"nr_unaccepted",
 #endif
+	vmstat_text_unused, /*SUSE_KABI_ZONE_STAT_PADDING1*/
+	vmstat_text_unused, /*SUSE_KABI_ZONE_STAT_PADDING2*/
 
 	/* enum numa_stat_item counters */
 #ifdef CONFIG_NUMA
@@ -1193,6 +1197,8 @@ const char * const vmstat_text[] = {
 	"numa_interleave",
 	"numa_local",
 	"numa_other",
+	vmstat_text_unused, /*SUSE_KABI_NUMA_STAT_PADDING1*/
+	vmstat_text_unused, /*SUSE_KABI_NUMA_STAT_PADDING2*/
 #endif
 
 	/* enum node_stat_item counters */
@@ -1246,6 +1252,11 @@ const char * const vmstat_text[] = {
 	"pgpromote_success",
 	"pgpromote_candidate",
 #endif
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING1*/
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING2*/
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING3*/
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING4*/
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING5*/
 
 	/* enum writeback_stat_item counters */
 	"nr_dirty_threshold",
@@ -1677,7 +1688,7 @@ static void zoneinfo_show_print(struct s
 	seq_printf(m, "Node %d, zone %8s", pgdat->node_id, zone->name);
 	if (is_zone_first_populated(pgdat, zone)) {
 		seq_printf(m, "\n  per-node stats");
-		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 			unsigned long pages = node_page_state_pages(pgdat, i);
 
 			if (vmstat_item_print_in_thp(i))
@@ -1719,12 +1730,12 @@ static void zoneinfo_show_print(struct s
 		return;
 	}
 
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++)
 		seq_printf(m, "\n      %-12s %lu", zone_stat_name(i),
 			   zone_page_state(zone, i));
 
 #ifdef CONFIG_NUMA
-	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)
+	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++)
 		seq_printf(m, "\n      %-12s %lu", numa_stat_name(i),
 			   zone_numa_event_state(zone, i));
 #endif
@@ -1800,17 +1811,17 @@ static void *vmstat_start(struct seq_fil
 	m->private = v;
 	if (!v)
 		return ERR_PTR(-ENOMEM);
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++)
 		v[i] = global_zone_page_state(i);
 	v += NR_VM_ZONE_STAT_ITEMS;
 
 #ifdef CONFIG_NUMA
-	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)
+	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++)
 		v[i] = global_numa_event_state(i);
 	v += NR_VM_NUMA_EVENT_ITEMS;
 #endif
 
-	for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 		v[i] = global_node_page_state_pages(i);
 		if (vmstat_item_print_in_thp(i))
 			v[i] /= HPAGE_PMD_NR;
@@ -1842,6 +1853,9 @@ static int vmstat_show(struct seq_file *
 	unsigned long *l = arg;
 	unsigned long off = l - (unsigned long *)m->private;
 
+	if (vmstat_text[off] == vmstat_text_unused)
+		return 0;
+
 	seq_puts(m, vmstat_text[off]);
 	seq_put_decimal_ull(m, " ", *l);
 	seq_putc(m, '\n');
@@ -1902,7 +1916,7 @@ int vmstat_refresh(struct ctl_table *tab
 	err = schedule_on_each_cpu(refresh_vm_stats);
 	if (err)
 		return err;
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++) {
 		/*
 		 * Skip checking stats known to go negative occasionally.
 		 */
@@ -1917,7 +1931,7 @@ int vmstat_refresh(struct ctl_table *tab
 				__func__, zone_stat_name(i), val);
 		}
 	}
-	for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 		/*
 		 * Skip checking stats known to go negative occasionally.
 		 */
