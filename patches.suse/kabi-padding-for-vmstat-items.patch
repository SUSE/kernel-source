From: Vlastimil Babka <vbabka@suse.cz>
Subject: kabi padding for vmstat items
Patch-mainline: Never, kabi placeholder
References: bsc#1220507

Adding new zone/node accounting fields is difficult without breaking kabi so
let's add some placeholder values to all the related enums.

The printing code is adjusted to skip them either by using a lower max index or
relying on the human readable names in vmstat_text pointing to a specific string
vmstat_text_unused with the contents "unused_counter".

This should be robust so in case any 3rd party module code or future backport
uses the full max index, it will just produce extra output with
"unused_counter" and zero value.

Where possible, counteris folding and other manipulation code is also adjusted
to not handle the unused entries.

Signed-off-by: Vlastimil Babka <vbabka@suse.cz>

---
 drivers/base/node.c    |    6 ++--
 include/linux/mmzone.h |   19 ++++++++++++++
 mm/memcontrol.c        |    2 -
 mm/vmstat.c            |   64 +++++++++++++++++++++++++++++--------------------
 4 files changed, 62 insertions(+), 29 deletions(-)

--- a/drivers/base/node.c
+++ b/drivers/base/node.c
@@ -521,20 +521,20 @@ static ssize_t node_read_vmstat(struct d
 	int i;
 	int len = 0;
 
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++)
 		len += sysfs_emit_at(buf, len, "%s %lu\n",
 				     zone_stat_name(i),
 				     sum_zone_node_page_state(nid, i));
 
 #ifdef CONFIG_NUMA
 	fold_vm_numa_events();
-	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)
+	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++)
 		len += sysfs_emit_at(buf, len, "%s %lu\n",
 				     numa_stat_name(i),
 				     sum_zone_numa_event_state(nid, i));
 
 #endif
-	for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 		unsigned long pages = node_page_state_pages(pgdat, i);
 
 		if (vmstat_item_print_in_thp(i))
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -129,10 +129,16 @@ enum numa_stat_item {
 	NUMA_INTERLEAVE_HIT,	/* interleaver preferred this zone */
 	NUMA_LOCAL,		/* allocation from local node */
 	NUMA_OTHER,		/* allocation from other node */
+	SUSE_KABI_NUMA_STAT_PADDING1,
+#ifndef __GENKSYMS__
+	NR_VM_NUMA_EVENT_ITEMS_USED = SUSE_KABI_NUMA_STAT_PADDING1,
+#endif
+	SUSE_KABI_NUMA_STAT_PADDING2,
 	NR_VM_NUMA_EVENT_ITEMS
 };
 #else
 #define NR_VM_NUMA_EVENT_ITEMS 0
+#define NR_VM_NUMA_EVENT_ITEMS_USED 0
 #endif
 
 enum zone_stat_item {
@@ -156,6 +162,11 @@ enum zone_stat_item {
 #ifdef CONFIG_UNACCEPTED_MEMORY
 	NR_UNACCEPTED,
 #endif
+	SUSE_KABI_ZONE_STAT_PADDING1,
+#ifndef __GENKSYMS__
+	NR_VM_ZONE_STAT_ITEMS_USED = SUSE_KABI_ZONE_STAT_PADDING1,
+#endif
+	SUSE_KABI_ZONE_STAT_PADDING2,
 	NR_VM_ZONE_STAT_ITEMS };
 
 enum node_stat_item {
@@ -221,6 +232,14 @@ enum node_stat_item {
 	PGDEMOTE_KSWAPD,
 	PGDEMOTE_DIRECT,
 	PGDEMOTE_KHUGEPAGED,
+	SUSE_KABI_NODE_STAT_PADDING1,
+#ifndef __GENKSYMS__
+	NR_VM_NODE_STAT_ITEMS_USED = SUSE_KABI_NODE_STAT_PADDING1,
+#endif
+	SUSE_KABI_NODE_STAT_PADDING2,
+	SUSE_KABI_NODE_STAT_PADDING3,
+	SUSE_KABI_NODE_STAT_PADDING4,
+	SUSE_KABI_NODE_STAT_PADDING5,
 	NR_VM_NODE_STAT_ITEMS
 };
 
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -4228,7 +4228,7 @@ static int memory_numa_stat_show(struct
 	for (i = 0; i < ARRAY_SIZE(memory_stats); i++) {
 		int nid;
 
-		if (memory_stats[i].idx >= NR_VM_NODE_STAT_ITEMS)
+		if (memory_stats[i].idx >= NR_VM_NODE_STAT_ITEMS_USED)
 			continue;
 
 		seq_printf(m, "%s", memory_stats[i].name);
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -168,7 +168,7 @@ EXPORT_SYMBOL(vm_node_stat);
 #ifdef CONFIG_NUMA
 static void fold_vm_zone_numa_events(struct zone *zone)
 {
-	unsigned long zone_numa_events[NR_VM_NUMA_EVENT_ITEMS] = { 0, };
+	unsigned long zone_numa_events[NR_VM_NUMA_EVENT_ITEMS_USED] = { 0, };
 	int cpu;
 	enum numa_stat_item item;
 
@@ -176,11 +176,11 @@ static void fold_vm_zone_numa_events(str
 		struct per_cpu_zonestat *pzstats;
 
 		pzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);
-		for (item = 0; item < NR_VM_NUMA_EVENT_ITEMS; item++)
+		for (item = 0; item < NR_VM_NUMA_EVENT_ITEMS_USED; item++)
 			zone_numa_events[item] += xchg(&pzstats->vm_numa_event[item], 0);
 	}
 
-	for (item = 0; item < NR_VM_NUMA_EVENT_ITEMS; item++)
+	for (item = 0; item < NR_VM_NUMA_EVENT_ITEMS_USED; item++)
 		zone_numa_event_add(zone_numa_events[item], zone, item);
 }
 
@@ -775,13 +775,13 @@ static int fold_diff(int *zone_diff, int
 	int i;
 	int changes = 0;
 
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++)
 		if (zone_diff[i]) {
 			atomic_long_add(zone_diff[i], &vm_zone_stat[i]);
 			changes++;
 	}
 
-	for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++)
 		if (node_diff[i]) {
 			atomic_long_add(node_diff[i], &vm_node_stat[i]);
 			changes++;
@@ -810,15 +810,15 @@ static int refresh_cpu_vm_stats(bool do_
 	struct pglist_data *pgdat;
 	struct zone *zone;
 	int i;
-	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };
-	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };
+	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS_USED] = { 0, };
+	int global_node_diff[NR_VM_NODE_STAT_ITEMS_USED] = { 0, };
 	int changes = 0;
 
 	for_each_populated_zone(zone) {
 		struct per_cpu_zonestat __percpu *pzstats = zone->per_cpu_zonestats;
 		struct per_cpu_pages __percpu *pcp = zone->per_cpu_pageset;
 
-		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++) {
 			int v;
 
 			v = this_cpu_xchg(pzstats->vm_stat_diff[i], 0);
@@ -873,7 +873,7 @@ static int refresh_cpu_vm_stats(bool do_
 	for_each_online_pgdat(pgdat) {
 		struct per_cpu_nodestat __percpu *p = pgdat->per_cpu_nodestats;
 
-		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 			int v;
 
 			v = this_cpu_xchg(p->vm_node_stat_diff[i], 0);
@@ -898,15 +898,15 @@ void cpu_vm_stats_fold(int cpu)
 	struct pglist_data *pgdat;
 	struct zone *zone;
 	int i;
-	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };
-	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };
+	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS_USED] = { 0, };
+	int global_node_diff[NR_VM_NODE_STAT_ITEMS_USED] = { 0, };
 
 	for_each_populated_zone(zone) {
 		struct per_cpu_zonestat *pzstats;
 
 		pzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);
 
-		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++) {
 			if (pzstats->vm_stat_diff[i]) {
 				int v;
 
@@ -917,7 +917,7 @@ void cpu_vm_stats_fold(int cpu)
 			}
 		}
 #ifdef CONFIG_NUMA
-		for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++) {
 			if (pzstats->vm_numa_event[i]) {
 				unsigned long v;
 
@@ -934,7 +934,7 @@ void cpu_vm_stats_fold(int cpu)
 
 		p = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu);
 
-		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++)
+		for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++)
 			if (p->vm_node_stat_diff[i]) {
 				int v;
 
@@ -957,7 +957,7 @@ void drain_zonestat(struct zone *zone, s
 	unsigned long v;
 	int i;
 
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++) {
 		if (pzstats->vm_stat_diff[i]) {
 			v = pzstats->vm_stat_diff[i];
 			pzstats->vm_stat_diff[i] = 0;
@@ -966,7 +966,7 @@ void drain_zonestat(struct zone *zone, s
 	}
 
 #ifdef CONFIG_NUMA
-	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++) {
 		if (pzstats->vm_numa_event[i]) {
 			v = pzstats->vm_numa_event[i];
 			pzstats->vm_numa_event[i] = 0;
@@ -1187,6 +1187,8 @@ int fragmentation_index(struct zone *zon
 					TEXT_FOR_HIGHMEM(xx) xx "_movable", \
 					TEXT_FOR_DEVICE(xx)
 
+static const char vmstat_text_unused[] = "unused_counter";
+
 const char * const vmstat_text[] = {
 	/* enum zone_stat_item counters */
 	"nr_free_pages",
@@ -1206,6 +1208,8 @@ const char * const vmstat_text[] = {
 #ifdef CONFIG_UNACCEPTED_MEMORY
 	"nr_unaccepted",
 #endif
+	vmstat_text_unused, /*SUSE_KABI_ZONE_STAT_PADDING1*/
+	vmstat_text_unused, /*SUSE_KABI_ZONE_STAT_PADDING2*/
 
 	/* enum numa_stat_item counters */
 #ifdef CONFIG_NUMA
@@ -1215,6 +1219,8 @@ const char * const vmstat_text[] = {
 	"numa_interleave",
 	"numa_local",
 	"numa_other",
+	vmstat_text_unused, /*SUSE_KABI_NUMA_STAT_PADDING1*/
+	vmstat_text_unused, /*SUSE_KABI_NUMA_STAT_PADDING2*/
 #endif
 
 	/* enum node_stat_item counters */
@@ -1274,6 +1280,12 @@ const char * const vmstat_text[] = {
 	"pgdemote_kswapd",
 	"pgdemote_direct",
 	"pgdemote_khugepaged",
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING1*/
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING2*/
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING3*/
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING4*/
+	vmstat_text_unused, /*SUSE_KABI_NODE_STAT_PADDING5*/
+
 	/* system-wide enum vm_stat_item counters */
 	"nr_dirty_threshold",
 	"nr_dirty_background_threshold",
@@ -1356,7 +1368,6 @@ const char * const vmstat_text[] = {
 	"compact_daemon_migrate_scanned",
 	"compact_daemon_free_scanned",
 #endif
-
 #ifdef CONFIG_HUGETLB_PAGE
 	"htlb_buddy_alloc_success",
 	"htlb_buddy_alloc_fail",
@@ -1732,7 +1743,7 @@ static void zoneinfo_show_print(struct s
 	seq_printf(m, "Node %d, zone %8s", pgdat->node_id, zone->name);
 	if (is_zone_first_populated(pgdat, zone)) {
 		seq_printf(m, "\n  per-node stats");
-		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+		for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 			unsigned long pages = node_page_state_pages(pgdat, i);
 
 			if (vmstat_item_print_in_thp(i))
@@ -1776,12 +1787,12 @@ static void zoneinfo_show_print(struct s
 		return;
 	}
 
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++)
 		seq_printf(m, "\n      %-12s %lu", zone_stat_name(i),
 			   zone_page_state(zone, i));
 
 #ifdef CONFIG_NUMA
-	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)
+	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++)
 		seq_printf(m, "\n      %-12s %lu", numa_stat_name(i),
 			   zone_numa_event_state(zone, i));
 #endif
@@ -1857,17 +1868,17 @@ static void *vmstat_start(struct seq_fil
 	m->private = v;
 	if (!v)
 		return ERR_PTR(-ENOMEM);
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++)
 		v[i] = global_zone_page_state(i);
 	v += NR_VM_ZONE_STAT_ITEMS;
 
 #ifdef CONFIG_NUMA
-	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)
+	for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS_USED; i++)
 		v[i] = global_numa_event_state(i);
 	v += NR_VM_NUMA_EVENT_ITEMS;
 #endif
 
-	for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 		v[i] = global_node_page_state_pages(i);
 		if (vmstat_item_print_in_thp(i))
 			v[i] /= HPAGE_PMD_NR;
@@ -1901,6 +1912,9 @@ static int vmstat_show(struct seq_file *
 	unsigned long *l = arg;
 	unsigned long off = l - (unsigned long *)m->private;
 
+	if (vmstat_text[off] == vmstat_text_unused)
+		return 0;
+
 	seq_puts(m, vmstat_text[off]);
 	seq_put_decimal_ull(m, " ", *l);
 	seq_putc(m, '\n');
@@ -1961,7 +1975,7 @@ int vmstat_refresh(const struct ctl_tabl
 	err = schedule_on_each_cpu(refresh_vm_stats);
 	if (err)
 		return err;
-	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS_USED; i++) {
 		/*
 		 * Skip checking stats known to go negative occasionally.
 		 */
@@ -1976,7 +1990,7 @@ int vmstat_refresh(const struct ctl_tabl
 				__func__, zone_stat_name(i), val);
 		}
 	}
-	for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
+	for (i = 0; i < NR_VM_NODE_STAT_ITEMS_USED; i++) {
 		/*
 		 * Skip checking stats known to go negative occasionally.
 		 */
