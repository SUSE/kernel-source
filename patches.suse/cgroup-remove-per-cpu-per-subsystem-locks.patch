From: Shakeel Butt <shakeel.butt@linux.dev>
Date: Tue, 17 Jun 2025 12:57:24 -0700
Subject: cgroup: remove per-cpu per-subsystem locks
Git-commit: 6af89c6ca71742e9227e6f8172a86ce1ee16aa85
Patch-mainline: v6.16 or v6.16-rc5 (next release)
References: bsc#1247963

The rstat update side used to insert the cgroup whose stats are updated
in the update tree and the read side flush the update tree to get the
latest uptodate stats. The per-cpu per-subsystem locks were used to
synchronize the update and flush side. However now the update side does
not access update tree but uses per-cpu lockless lists. So there is no
need for locks to synchronize update and flush side. Let's remove them.

Suggested-by: JP Kobryn <inwardvessel@gmail.com>
Signed-off-by: Shakeel Butt <shakeel.butt@linux.dev>
Tested-by: JP Kobryn <inwardvessel@gmail.com>
Signed-off-by: Tejun Heo <tj@kernel.org>
[ ptesarik: Since SUSE-2024 has not backported per-subsystem locks,
  only the global per-cpu lock is removed. ]
Acked-by: Petr Tesarik <ptesarik@suse.com>
---
 kernel/cgroup/rstat.c |   22 +++-------------------
 1 file changed, 3 insertions(+), 19 deletions(-)

--- a/kernel/cgroup/rstat.c
+++ b/kernel/cgroup/rstat.c
@@ -8,7 +8,6 @@
 #include <linux/btf_ids.h>
 
 static DEFINE_SPINLOCK(cgroup_rstat_lock);
-static DEFINE_PER_CPU(raw_spinlock_t, cgroup_rstat_cpu_lock);
 static DEFINE_PER_CPU(struct llist_head, rstat_backlog_list);
 
 static void cgroup_base_stat_flush(struct cgroup *cgrp, int cpu);
@@ -185,26 +184,14 @@ next_level:
  */
 static struct cgroup *cgroup_rstat_updated_list(struct cgroup *root, int cpu)
 {
-	raw_spinlock_t *cpu_lock = per_cpu_ptr(&cgroup_rstat_cpu_lock, cpu);
 	struct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(root, cpu);
 	struct cgroup *head = NULL, *parent, *child;
-	unsigned long flags;
-
-	/*
-	 * The _irqsave() is needed because cgroup_rstat_lock is
-	 * spinlock_t which is a sleeping lock on PREEMPT_RT. Acquiring
-	 * this lock with the _irq() suffix only disables interrupts on
-	 * a non-PREEMPT_RT kernel. The raw_spinlock_t below disables
-	 * interrupts on both configurations. The _irqsave() ensures
-	 * that interrupts are always disabled and later restored.
-	 */
-	raw_spin_lock_irqsave(cpu_lock, flags);
 
 	cgroup_process_update_tree(cpu);
 
 	/* Return NULL if this subtree is not on-list */
 	if (!rstatc->updated_next)
-		goto unlock_ret;
+		return NULL;
 
 	/*
 	 * Unlink @root from its parent. As the updated_children list is
@@ -236,8 +223,7 @@ static struct cgroup *cgroup_rstat_updat
 	rstatc->updated_children = root;
 	if (child != root)
 		head = cgroup_rstat_push_children(head, child, cpu);
-unlock_ret:
-	raw_spin_unlock_irqrestore(cpu_lock, flags);
+
 	return head;
 }
 
@@ -394,10 +380,8 @@ void __init cgroup_rstat_boot(void)
 {
 	int cpu;
 
-	for_each_possible_cpu(cpu) {
-		raw_spin_lock_init(per_cpu_ptr(&cgroup_rstat_cpu_lock, cpu));
+	for_each_possible_cpu(cpu)
 		init_llist_head(per_cpu_ptr(&rstat_backlog_list, cpu));
-	}
 }
 
 /*
