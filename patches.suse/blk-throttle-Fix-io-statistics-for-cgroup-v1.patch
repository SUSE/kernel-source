From: Jinke Han <hanjinke.666@bytedance.com>
Date: Mon, 8 May 2023 01:06:31 +0800
Subject: blk-throttle: Fix io statistics for cgroup v1
Git-commit: ad7c3b41e86b59943a903d23c7b037d820e6270c
Patch-mainline: v6.5-rc1
References: bsc#1233528

After commit f382fb0bcef4 ("block: remove legacy IO schedulers"),
blkio.throttle.io_serviced and blkio.throttle.io_service_bytes become
the only stable io stats interface of cgroup v1, and these statistics
are done in the blk-throttle code. But the current code only counts the
bios that are actually throttled. When the user does not add the throttle
limit, the io stats for cgroup v1 has nothing. I fix it according to the
statistical method of v2, and made it count all ios accurately.

Fixes: a7b36ee6ba29 ("block: move blk-throtl fast path inline")
Tested-by: Andrea Righi <andrea.righi@canonical.com>
Signed-off-by: Jinke Han <hanjinke.666@bytedance.com>
Acked-by: Muchun Song <songmuchun@bytedance.com>
Acked-by: Tejun Heo <tj@kernel.org>
Link: https://lore.kernel.org/r/20230507170631.89607-1-hanjinke.666@bytedance.com
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Acked-by: Michal Koutn√Ω <mkoutny@suse.com>
[ mkoutny: Adjust for missing
  81c7a63abc7c0 ("blk-throttle: improve bypassing bios checkings") v6.1-rc1~134^2~26
  3b8cc62987240 ("blk-cgroup: Optimize blkcg_rstat_flush()") v6.2-rc1~129^2~68
]
---
 block/blk-cgroup.c   |    6 ++++--
 block/blk-throttle.c |    6 ------
 block/blk-throttle.h |    9 +++++++++
 3 files changed, 13 insertions(+), 8 deletions(-)

--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -2008,6 +2008,9 @@ void blk_cgroup_bio_start(struct bio *bi
 	struct blkg_iostat_set *bis;
 	unsigned long flags;
 
+	if (!cgroup_subsys_on_dfl(io_cgrp_subsys))
+		return;
+
 	cpu = get_cpu();
 	bis = per_cpu_ptr(bio->bi_blkg->iostat_cpu, cpu);
 	flags = u64_stats_update_begin_irqsave(&bis->sync);
@@ -2023,8 +2026,7 @@ void blk_cgroup_bio_start(struct bio *bi
 	bis->cur.ios[rwd]++;
 
 	u64_stats_update_end_irqrestore(&bis->sync, flags);
-	if (cgroup_subsys_on_dfl(io_cgrp_subsys))
-		cgroup_rstat_updated(bio->bi_blkg->blkcg->css.cgroup, cpu);
+	cgroup_rstat_updated(bio->bi_blkg->blkcg->css.cgroup, cpu);
 	put_cpu();
 }
 
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -2091,12 +2091,6 @@ bool __blk_throtl_bio(struct bio *bio)
 
 	rcu_read_lock();
 
-	if (!cgroup_subsys_on_dfl(io_cgrp_subsys)) {
-		blkg_rwstat_add(&tg->stat_bytes, bio->bi_opf,
-				bio->bi_iter.bi_size);
-		blkg_rwstat_add(&tg->stat_ios, bio->bi_opf, 1);
-	}
-
 	spin_lock_irq(&q->queue_lock);
 
 	throtl_update_latency_buckets(td);
--- a/block/blk-throttle.h
+++ b/block/blk-throttle.h
@@ -174,6 +174,15 @@ static inline bool blk_throtl_bio(struct
 {
 	struct throtl_grp *tg = blkg_to_tg(bio->bi_blkg);
 
+	if (!cgroup_subsys_on_dfl(io_cgrp_subsys)) {
+		if (!bio_flagged(bio, BIO_CGROUP_ACCT)) {
+			bio_set_flag(bio, BIO_CGROUP_ACCT);
+			blkg_rwstat_add(&tg->stat_bytes, bio->bi_opf,
+					bio->bi_iter.bi_size);
+		}
+		blkg_rwstat_add(&tg->stat_ios, bio->bi_opf, 1);
+	}
+
 	/* no need to throttle bps any more if the bio has been throttled */
 	if (bio_flagged(bio, BIO_BPS_THROTTLED) &&
 	    !(tg->flags & THROTL_TG_HAS_IOPS_LIMIT))
