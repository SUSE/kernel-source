From: Linus Torvalds <torvalds@linux-foundation.org>
Date: Wed, 11 Sep 2024 17:11:23 -0700
Subject: mm: avoid leaving partial pfn mappings around in error case
Git-commit: 79a61cc3fc0466ad2b7b89618a6157785f0293b3
Patch-mainline: v6.11
References: CVE-2024-47674 bsc#1231673

As Jann points out, PFN mappings are special, because unlike normal
memory mappings, there is no lifetime information associated with the
mapping - it is just a raw mapping of PFNs with no reference counting of
a 'struct page'.

That's all very much intentional, but it does mean that it's easy to
mess up the cleanup in case of errors.  Yes, a failed mmap() will always
eventually clean up any partial mappings, but without any explicit
lifetime in the page table mapping itself, it's very easy to do the
error handling in the wrong order.

In particular, it's easy to mistakenly free the physical backing store
before the page tables are actually cleaned up and (temporarily) have
stale dangling PTE entries.

To make this situation less error-prone, just make sure that any partial
pfn mapping is torn down early, before any other error handling.

Reported-and-tested-by: Jann Horn <jannh@google.com>
Cc: Andrew Morton <akpm@linux-foundation.org>
Cc: Jason Gunthorpe <jgg@ziepe.ca>
Cc: Simona Vetter <simona.vetter@ffwll.ch>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
---
 mm/memory.c |   10 +++++++++-
 1 file changed, 9 insertions(+), 1 deletion(-)

--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1910,6 +1910,7 @@ int remap_pfn_range(struct vm_area_struc
 {
 	pgd_t *pgd;
 	unsigned long next;
+	unsigned long start = addr;
 	unsigned long end = addr + PAGE_ALIGN(size);
 	struct mm_struct *mm = vma->vm_mm;
 	unsigned long remap_pfn = pfn;
@@ -1957,8 +1958,15 @@ int remap_pfn_range(struct vm_area_struc
 			break;
 	} while (pgd++, addr = next, addr != end);
 
-	if (err)
+	if (err) {
+		/*
+		 * A partial pfn range mapping is dangerous: it does not
+		 * maintain page reference counts, and callers may free
+		 * pages due to the error. So zap it early.
+		 */
+		zap_page_range_single(vma, start, size, NULL);
 		untrack_pfn(vma, remap_pfn, PAGE_ALIGN(size));
+	}
 
 	return err;
 }
