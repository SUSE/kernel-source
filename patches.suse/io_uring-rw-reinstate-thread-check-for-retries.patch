From 039a2e800bcd5beb89909d1a488abf3d647642cf Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Thu, 25 Apr 2024 09:04:32 -0600
Subject: [PATCH] io_uring/rw: reinstate thread check for retries
Git-commit: 039a2e800bcd5beb89909d1a488abf3d647642cf
Patch-mainline: v6.10-rc1
References: bsc#1230569

Allowing retries for everything is arguably the right thing to do, now
that every command type is async read from the start. But it's exposed a
few issues around missing check for a retry (which cca6571381a0 exposed),
and the fixup commit for that isn't necessarily 100% sound in terms of
iov_iter state.

For now, just revert these two commits. This unfortunately then re-opens
the fact that -EAGAIN can get bubbled to userspace for some cases where
the kernel very well could just sanely retry them. But until we have all
the conditions covered around that, we cannot safely enable that.

This reverts commit df604d2ad480fcf7b39767280c9093e13b1de952.
This reverts commit cca6571381a0bdc88021a1f7a4c2349df21279f7.

Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Gabriel Krisman Bertazi <krisman@suse.de>
---
 io_uring/io_uring.c |   13 -------------
 io_uring/io_uring.h |    1 -
 io_uring/rw.c       |   40 +++++++++++++++++++++++++++++-----------
 3 files changed, 29 insertions(+), 25 deletions(-)

--- a/io_uring/io_uring.c
+++ b/io_uring/io_uring.c
@@ -520,19 +520,6 @@ static void io_queue_iowq(struct io_kioc
 		io_queue_linked_timeout(link);
 }
 
-static void io_tw_requeue_iowq(struct io_kiocb *req, struct io_tw_state *ts)
-{
-	req->flags &= ~REQ_F_REISSUE;
-	io_queue_iowq(req);
-}
-
-void io_tw_queue_iowq(struct io_kiocb *req)
-{
-	req->flags |= REQ_F_REISSUE | REQ_F_BL_NO_RECYCLE;
-	req->io_task_work.func = io_tw_requeue_iowq;
-	io_req_task_work_add(req);
-}
-
 static __cold void io_queue_deferred(struct io_ring_ctx *ctx)
 {
 	while (!list_empty(&ctx->defer_list)) {
--- a/io_uring/io_uring.h
+++ b/io_uring/io_uring.h
@@ -76,7 +76,6 @@ struct file *io_file_get_fixed(struct io
 void __io_req_task_work_add(struct io_kiocb *req, unsigned flags);
 bool io_alloc_async_data(struct io_kiocb *req);
 void io_req_task_queue(struct io_kiocb *req);
-void io_tw_queue_iowq(struct io_kiocb *req);
 void io_req_task_complete(struct io_kiocb *req, struct io_tw_state *ts);
 void io_req_task_queue_fail(struct io_kiocb *req, int ret);
 void io_req_task_submit(struct io_kiocb *req, struct io_tw_state *ts);
--- a/io_uring/rw.c
+++ b/io_uring/rw.c
@@ -418,9 +418,16 @@ static inline loff_t *io_kiocb_update_po
 	return NULL;
 }
 
+#ifdef CONFIG_BLOCK
+static void io_resubmit_prep(struct io_kiocb *req)
+{
+	struct io_async_rw *io = req->async_data;
+
+	iov_iter_restore(&io->iter, &io->iter_state);
+}
+
 static bool io_rw_should_reissue(struct io_kiocb *req)
 {
-#ifdef CONFIG_BLOCK
 	umode_t mode = file_inode(req->file)->i_mode;
 	struct io_ring_ctx *ctx = req->ctx;
 
@@ -436,11 +443,23 @@ static bool io_rw_should_reissue(struct
 	 */
 	if (percpu_ref_is_dying(&ctx->refs))
 		return false;
+	/*
+	 * Play it safe and assume not safe to re-import and reissue if we're
+	 * not in the original thread group (or in task context).
+	 */
+	if (!same_thread_group(req->task, current) || !in_task())
+		return false;
 	return true;
+}
 #else
+static void io_resubmit_prep(struct io_kiocb *req)
+{
+}
+static bool io_rw_should_reissue(struct io_kiocb *req)
+{
 	return false;
-#endif
 }
+#endif
 
 static void io_req_end_write(struct io_kiocb *req)
 {
@@ -477,7 +496,7 @@ static bool __io_complete_rw_common(stru
 			 * current cycle.
 			 */
 			io_req_io_end(req);
-			io_tw_queue_iowq(req);
+			req->flags |= REQ_F_REISSUE | REQ_F_BL_NO_RECYCLE;
 			return true;
 		}
 		req_set_fail(req);
@@ -543,7 +562,7 @@ static void io_complete_rw_iopoll(struct
 		io_req_end_write(req);
 	if (unlikely(res != req->cqe.res)) {
 		if (res == -EAGAIN && io_rw_should_reissue(req)) {
-			io_tw_queue_iowq(req);
+			req->flags |= REQ_F_REISSUE | REQ_F_BL_NO_RECYCLE;
 			return;
 		}
 		req->cqe.res = res;
@@ -578,10 +597,8 @@ static int kiocb_done(struct io_kiocb *r
 	}
 
 	if (req->flags & REQ_F_REISSUE) {
-		struct io_async_rw *io = req->async_data;
-
 		req->flags &= ~REQ_F_REISSUE;
-		iov_iter_restore(&io->iter, &io->iter_state);
+		io_resubmit_prep(req);
 		return -EAGAIN;
 	}
 	return IOU_ISSUE_SKIP_COMPLETE;
@@ -834,8 +851,7 @@ static int __io_read(struct io_kiocb *re
 	ret = io_iter_do_read(rw, &io->iter);
 
 	if (ret == -EAGAIN || (req->flags & REQ_F_REISSUE)) {
-		if (req->flags & REQ_F_REISSUE)
-			return IOU_ISSUE_SKIP_COMPLETE;
+		req->flags &= ~REQ_F_REISSUE;
 		/* If we can poll, just do that. */
 		if (io_file_can_poll(req))
 			return -EAGAIN;
@@ -1030,8 +1046,10 @@ int io_write(struct io_kiocb *req, unsig
 	else
 		ret2 = -EINVAL;
 
-	if (req->flags & REQ_F_REISSUE)
-		return IOU_ISSUE_SKIP_COMPLETE;
+	if (req->flags & REQ_F_REISSUE) {
+		req->flags &= ~REQ_F_REISSUE;
+		ret2 = -EAGAIN;
+	}
 
 	/*
 	 * Raw bdev writes will return -EOPNOTSUPP for IOCB_NOWAIT. Just
