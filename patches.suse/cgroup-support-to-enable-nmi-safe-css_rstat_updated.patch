From: Shakeel Butt <shakeel.butt@linux.dev>
Date: Tue, 17 Jun 2025 12:57:22 -0700
Subject: cgroup: support to enable nmi-safe css_rstat_updated
Git-commit: 1257b8786ac689a2ce5fe3e1741c65038035adc6
Patch-mainline: v6.17-rc1
References: bsc#1247963

Add necessary infrastructure to enable the nmi-safe execution of
css_rstat_updated(). Currently css_rstat_updated() takes a per-cpu
per-css raw spinlock to add the given css in the per-cpu per-css update
tree. However the kernel can not spin in nmi context, so we need to
remove the spinning on the raw spinlock in css_rstat_updated().

To support lockless css_rstat_updated(), let's add necessary data
structures in the css and ss structures.

Signed-off-by: Shakeel Butt <shakeel.butt@linux.dev>
Tested-by: JP Kobryn <inwardvessel@gmail.com>
Signed-off-by: Tejun Heo <tj@kernel.org>
[ ptesarik: SL-16.0 has not backported cgroup per-subsystem locks,
  so I'm routing everything through the backlog list. Since there is
  no ss_rstat_init(), move the allocation of llist_head per-cpu to
  cgroup_init_subsys(). ]
Signed-off-by: Petr Tesarik <ptesarik@suse.com>
---
 include/linux/cgroup-defs.h |    3 +++
 kernel/cgroup/rstat.c       |    8 ++++++--
 2 files changed, 9 insertions(+), 2 deletions(-)

--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -388,6 +388,9 @@ struct cgroup_rstat_cpu {
 	 */
 	struct cgroup *updated_children;	/* terminated by self cgroup */
 	struct cgroup *updated_next;		/* NULL iff not on the list */
+
+	struct llist_node lnode;		/* lockless list for update */
+	struct cgroup *owner;			/* back pointer */
 };
 
 struct cgroup_freezer_state {
--- a/kernel/cgroup/rstat.c
+++ b/kernel/cgroup/rstat.c
@@ -11,6 +11,7 @@
 
 static DEFINE_SPINLOCK(cgroup_rstat_lock);
 static DEFINE_PER_CPU(raw_spinlock_t, cgroup_rstat_cpu_lock);
+static DEFINE_PER_CPU(struct llist_head, rstat_backlog_list);
 
 static void cgroup_base_stat_flush(struct cgroup *cgrp, int cpu);
 
@@ -397,7 +398,8 @@ int cgroup_rstat_init(struct cgroup *cgr
 	for_each_possible_cpu(cpu) {
 		struct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);
 
-		rstatc->updated_children = cgrp;
+		rstatc->owner = rstatc->updated_children = cgrp;
+		init_llist_node(&rstatc->lnode);
 		u64_stats_init(&rstatc->bsync);
 	}
 
@@ -427,8 +429,10 @@ void __init cgroup_rstat_boot(void)
 {
 	int cpu;
 
-	for_each_possible_cpu(cpu)
+	for_each_possible_cpu(cpu) {
 		raw_spin_lock_init(per_cpu_ptr(&cgroup_rstat_cpu_lock, cpu));
+		init_llist_head(per_cpu_ptr(&rstat_backlog_list, cpu));
+	}
 }
 
 /*
