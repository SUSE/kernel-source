From cedfec5e620d24597280d22ca34f0a0eac010cdf Mon Sep 17 00:00:00 2001
From: Josh Don <joshdon@google.com>
Date: Fri, 22 Sep 2023 16:05:34 -0700
Subject: [PATCH] sched/fair: Make cfs_rq->throttled_csd_list available on !SMP

References: bsc#1212887 (Scheduler functional and performance backports)
Git-commit: 30797bce8ef0c73f0c388148ffac92458533b10e
Patch-mainline: v6.7-rc1

This makes the following patch cleaner by avoiding extra CONFIG_SMP
conditionals on the availability of rq->throttled_csd_list.

Signed-off-by: Josh Don <joshdon@google.com>
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20230922230535.296350-1-joshdon@google.com
Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 kernel/sched/fair.c  | 4 ----
 kernel/sched/sched.h | 2 --
 2 files changed, 6 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 37bb79a0d9af..988c71484be9 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5699,11 +5699,9 @@ static bool distribute_cfs_runtime(struct cfs_bandwidth *cfs_b)
 		if (!cfs_rq_throttled(cfs_rq))
 			goto next;
 
-#ifdef CONFIG_SMP
 		/* Already queued for async unthrottle */
 		if (!list_empty(&cfs_rq->throttled_csd_list))
 			goto next;
-#endif
 
 		/* By the above checks, this should never be true */
 		SCHED_WARN_ON(cfs_rq->runtime_remaining > 0);
@@ -6070,9 +6068,7 @@ static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 {
 	cfs_rq->runtime_enabled = 0;
 	INIT_LIST_HEAD(&cfs_rq->throttled_list);
-#ifdef CONFIG_SMP
 	INIT_LIST_HEAD(&cfs_rq->throttled_csd_list);
-#endif
 }
 
 void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 485fbda7321c..be59e32b3a86 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -644,9 +644,7 @@ struct cfs_rq {
 	int			throttled;
 	int			throttle_count;
 	struct list_head	throttled_list;
-#ifdef CONFIG_SMP
 	struct list_head	throttled_csd_list;
-#endif
 #endif /* CONFIG_CFS_BANDWIDTH */
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
