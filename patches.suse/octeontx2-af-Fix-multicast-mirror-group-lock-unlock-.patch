From: Suman Ghosh <sumang@marvell.com>
Date: Wed, 13 Dec 2023 15:23:49 +0530
Subject: octeontx2-af: Fix multicast/mirror group lock/unlock issue
Patch-mainline: v6.8-rc1
Git-commit: 10b7572d17871b027de1d17152f08a2dc9c3aef6
References: jsc#PED-6931

As per the existing implementation, there exists a race between finding
a multicast/mirror group entry and deleting that entry. The group lock
was taken and released independently by rvu_nix_mcast_find_grp_elem()
function. Which is incorrect and group lock should be taken during the
entire operation of group updation/deletion. This patch fixes the same.

Fixes: 51b2804c19cd ("octeontx2-af: Add new mbox to support multicast/mirror offload")
Signed-off-by: Suman Ghosh <sumang@marvell.com>
Reviewed-by: Simon Horman <horms@kernel.org>
Signed-off-by: David S. Miller <davem@davemloft.net>
Acked-by: Thomas Bogendoerfer <tbogendoerfer@suse.de>
---
 drivers/net/ethernet/marvell/octeontx2/af/rvu_nix.c |   84 ++++++++++++--------
 1 file changed, 54 insertions(+), 30 deletions(-)

--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu_nix.c
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_nix.c
@@ -6044,14 +6044,12 @@ static struct nix_mcast_grp_elem *rvu_ni
 	struct nix_mcast_grp_elem *iter;
 	bool is_found = false;
 
-	mutex_lock(&mcast_grp->mcast_grp_lock);
 	list_for_each_entry(iter, &mcast_grp->mcast_grp_head, list) {
 		if (iter->mcast_grp_idx == mcast_grp_idx) {
 			is_found = true;
 			break;
 		}
 	}
-	mutex_unlock(&mcast_grp->mcast_grp_lock);
 
 	if (is_found)
 		return iter;
@@ -6064,7 +6062,7 @@ int rvu_nix_mcast_get_mce_index(struct r
 	struct nix_mcast_grp_elem *elem;
 	struct nix_mcast_grp *mcast_grp;
 	struct nix_hw *nix_hw;
-	int blkaddr;
+	int blkaddr, ret;
 
 	blkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);
 	nix_hw = get_nix_hw(rvu->hw, blkaddr);
@@ -6072,11 +6070,15 @@ int rvu_nix_mcast_get_mce_index(struct r
 		return NIX_AF_ERR_INVALID_NIXBLK;
 
 	mcast_grp = &nix_hw->mcast_grp;
+	mutex_lock(&mcast_grp->mcast_grp_lock);
 	elem = rvu_nix_mcast_find_grp_elem(mcast_grp, mcast_grp_idx);
 	if (!elem)
-		return NIX_AF_ERR_INVALID_MCAST_GRP;
+		ret = NIX_AF_ERR_INVALID_MCAST_GRP;
+	else
+		ret = elem->mce_start_index;
 
-	return elem->mce_start_index;
+	mutex_unlock(&mcast_grp->mcast_grp_lock);
+	return ret;
 }
 
 void rvu_nix_mcast_flr_free_entries(struct rvu *rvu, u16 pcifunc)
@@ -6140,7 +6142,7 @@ int rvu_nix_mcast_update_mcam_entry(stru
 	struct nix_mcast_grp_elem *elem;
 	struct nix_mcast_grp *mcast_grp;
 	struct nix_hw *nix_hw;
-	int blkaddr;
+	int blkaddr, ret = 0;
 
 	blkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);
 	nix_hw = get_nix_hw(rvu->hw, blkaddr);
@@ -6148,13 +6150,15 @@ int rvu_nix_mcast_update_mcam_entry(stru
 		return NIX_AF_ERR_INVALID_NIXBLK;
 
 	mcast_grp = &nix_hw->mcast_grp;
+	mutex_lock(&mcast_grp->mcast_grp_lock);
 	elem = rvu_nix_mcast_find_grp_elem(mcast_grp, mcast_grp_idx);
 	if (!elem)
-		return NIX_AF_ERR_INVALID_MCAST_GRP;
-
-	elem->mcam_index = mcam_index;
+		ret = NIX_AF_ERR_INVALID_MCAST_GRP;
+	else
+		elem->mcam_index = mcam_index;
 
-	return 0;
+	mutex_unlock(&mcast_grp->mcast_grp_lock);
+	return ret;
 }
 
 int rvu_mbox_handler_nix_mcast_grp_create(struct rvu *rvu,
@@ -6199,18 +6203,27 @@ int rvu_mbox_handler_nix_mcast_grp_destr
 	struct npc_delete_flow_rsp uninstall_rsp = { 0 };
 	struct nix_mcast_grp_elem *elem;
 	struct nix_mcast_grp *mcast_grp;
+	int blkaddr, err, ret = 0;
 	struct nix_mcast *mcast;
 	struct nix_hw *nix_hw;
-	int blkaddr, err;
 
 	err = nix_get_struct_ptrs(rvu, req->hdr.pcifunc, &nix_hw, &blkaddr);
 	if (err)
 		return err;
 
 	mcast_grp = &nix_hw->mcast_grp;
+
+	/* If AF is requesting for the deletion,
+	 * then AF is already taking the lock
+	 */
+	if (!req->is_af)
+		mutex_lock(&mcast_grp->mcast_grp_lock);
+
 	elem = rvu_nix_mcast_find_grp_elem(mcast_grp, req->mcast_grp_idx);
-	if (!elem)
-		return NIX_AF_ERR_INVALID_MCAST_GRP;
+	if (!elem) {
+		ret = NIX_AF_ERR_INVALID_MCAST_GRP;
+		goto unlock_grp;
+	}
 
 	/* If no mce entries are associated with the group
 	 * then just remove it from the global list.
@@ -6235,19 +6248,15 @@ int rvu_mbox_handler_nix_mcast_grp_destr
 	mutex_unlock(&mcast->mce_lock);
 
 delete_grp:
-	/* If AF is requesting for the deletion,
-	 * then AF is already taking the lock
-	 */
-	if (!req->is_af)
-		mutex_lock(&mcast_grp->mcast_grp_lock);
-
 	list_del(&elem->list);
 	kfree(elem);
 	mcast_grp->count--;
+
+unlock_grp:
 	if (!req->is_af)
 		mutex_unlock(&mcast_grp->mcast_grp_lock);
 
-	return 0;
+	return ret;
 }
 
 int rvu_mbox_handler_nix_mcast_grp_update(struct rvu *rvu,
@@ -6272,9 +6281,18 @@ int rvu_mbox_handler_nix_mcast_grp_updat
 		return err;
 
 	mcast_grp = &nix_hw->mcast_grp;
+
+	/* If AF is requesting for the updation,
+	 * then AF is already taking the lock
+	 */
+	if (!req->is_af)
+		mutex_lock(&mcast_grp->mcast_grp_lock);
+
 	elem = rvu_nix_mcast_find_grp_elem(mcast_grp, req->mcast_grp_idx);
-	if (!elem)
-		return NIX_AF_ERR_INVALID_MCAST_GRP;
+	if (!elem) {
+		ret = NIX_AF_ERR_INVALID_MCAST_GRP;
+		goto unlock_grp;
+	}
 
 	/* If any pcifunc matches the group's pcifunc, then we can
 	 * delete the entire group.
@@ -6285,9 +6303,10 @@ int rvu_mbox_handler_nix_mcast_grp_updat
 				/* Delete group */
 				dreq.hdr.pcifunc = elem->pcifunc;
 				dreq.mcast_grp_idx = elem->mcast_grp_idx;
-				dreq.is_af = req->is_af;
+				dreq.is_af = 1;
 				rvu_mbox_handler_nix_mcast_grp_destroy(rvu, &dreq, NULL);
-				return 0;
+				ret = 0;
+				goto unlock_grp;
 			}
 		}
 	}
@@ -6312,7 +6331,7 @@ int rvu_mbox_handler_nix_mcast_grp_updat
 				npc_enable_mcam_entry(rvu, mcam, npc_blkaddr,
 						      elem->mcam_index, true);
 				ret = NIX_AF_ERR_NON_CONTIG_MCE_LIST;
-				goto done;
+				goto unlock_mce;
 			}
 		}
 
@@ -6328,7 +6347,7 @@ int rvu_mbox_handler_nix_mcast_grp_updat
 				npc_enable_mcam_entry(rvu, mcam, npc_blkaddr,
 						      elem->mcam_index, true);
 
-			goto done;
+			goto unlock_mce;
 		}
 	} else {
 		if (!prev_count || prev_count < req->num_mce_entry) {
@@ -6336,7 +6355,7 @@ int rvu_mbox_handler_nix_mcast_grp_updat
 				npc_enable_mcam_entry(rvu, mcam, npc_blkaddr,
 						      elem->mcam_index, true);
 			ret = NIX_AF_ERR_INVALID_MCAST_DEL_REQ;
-			goto done;
+			goto unlock_mce;
 		}
 
 		nix_free_mce_list(mcast, prev_count, elem->mce_start_index, elem->dir);
@@ -6352,14 +6371,14 @@ int rvu_mbox_handler_nix_mcast_grp_updat
 						      elem->mcam_index,
 						      true);
 
-			goto done;
+			goto unlock_mce;
 		}
 	}
 
 	if (elem->mcam_index == -1) {
 		rsp->mce_start_index = elem->mce_start_index;
 		ret = 0;
-		goto done;
+		goto unlock_mce;
 	}
 
 	nix_mcast_update_action(rvu, elem);
@@ -6367,7 +6386,12 @@ int rvu_mbox_handler_nix_mcast_grp_updat
 	rsp->mce_start_index = elem->mce_start_index;
 	ret = 0;
 
-done:
+unlock_mce:
 	mutex_unlock(&mcast->mce_lock);
+
+unlock_grp:
+	if (!req->is_af)
+		mutex_unlock(&mcast_grp->mcast_grp_lock);
+
 	return ret;
 }
