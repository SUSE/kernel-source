From 6a4d1ff7495596516e7d255dbd60294a47e333ff Mon Sep 17 00:00:00 2001
From: Mel Gorman <mgorman@techsingularity.net>
Date: Fri, 18 Jul 2025 12:22:49 +0100
Subject: [PATCH] sched/fair: Workaround NO_RUN_TO_PARITY fix kabi

References: bsc#1234634 (Scheduler functional and performance backports)
Patch-mainline: Never, kABI

For clarity, vlag and vprot share the same space in the sched_entity
struct but this is a KABI hazard. Restore the old structure when
checking symbols. This is relevant to the following patches

  patches.suse/sched-fair-Fix-NO_RUN_TO_PARITY-case.patch
  patches.suse/sched-fair-Fix-entity-s-lag-with-run-to-parity.patch
  patches.suse/sched-fair-Limit-run-to-parity-to-the-min-slice-of-enqueued-entities.patch

Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 include/linux/sched.h | 10 ++++++++++
 kernel/sched/fair.c   |  8 ++++----
 2 files changed, 14 insertions(+), 4 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3727419a4d7a..cd379932f74a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -548,6 +548,12 @@ struct sched_statistics {
 #endif /* CONFIG_SCHEDSTATS */
 } ____cacheline_aligned;
 
+#ifndef __GENKSYMS__
+#define se_vprot(s) s->vprot
+#else
+#define se_vprot(s) s->vlag
+#endif
+
 struct sched_entity {
 	/* For load-balancing: */
 	struct load_weight		load;
@@ -567,6 +573,7 @@ struct sched_entity {
 	u64				sum_exec_runtime;
 	u64				prev_sum_exec_runtime;
 	u64				vruntime;
+#ifndef __GENKSYMS__
 	union {
 		/*
 		 * When !@on_rq this field is vlag.
@@ -576,6 +583,9 @@ struct sched_entity {
 		s64                     vlag;
 		u64                     vprot;
 	};
+#else
+	s64				vlag;
+#endif /* __GENKSYMS */
 	u64				slice;
 
 	u64				nr_migrations;
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 4cfb39d60820..eb7023de50aa 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -903,25 +903,25 @@ static inline void set_protect_slice(struct cfs_rq *cfs_rq, struct sched_entity
 	if (slice != se->slice)
 		vprot = min_vruntime(vprot, se->vruntime + calc_delta_fair(slice, se));
 
-	se->vprot = vprot;
+	se_vprot(se) = (u64)vprot;
 }
 
 static inline void update_protect_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)
 {
 	u64 slice = cfs_rq_min_slice(cfs_rq);
 
-	se->vprot = min_vruntime(se->vprot, se->vruntime + calc_delta_fair(slice, se));
+	se_vprot(se) = (u64)min_vruntime((u64)se_vprot(se), se->vruntime + calc_delta_fair(slice, se));
 }
 
 static inline bool protect_slice(struct sched_entity *se)
 {
-	return ((s64)(se->vprot - se->vruntime) > 0);
+	return ((s64)((u64)se_vprot(se) - se->vruntime) > 0);
 }
 
 static inline void cancel_protect_slice(struct sched_entity *se)
 {
 	if (protect_slice(se))
-		se->vprot = se->vruntime;
+		se_vprot(se) = (u64)se->vruntime;
 }
 
 /*
