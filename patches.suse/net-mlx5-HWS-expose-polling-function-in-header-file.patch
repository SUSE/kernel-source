From: Yevgeny Kliteynik <kliteyn@nvidia.com>
Date: Sun, 11 May 2025 22:38:03 +0300
Subject: net/mlx5: HWS, expose polling function in header file
Patch-mainline: v6.16-rc1
Git-commit: 3c739d1624e3c3186a0a0248e91851a085f6e45b
References: jsc#PED-14197 jsc#PED-14199 jsc#PED-15315

In preparation for complex matcher, expose the function that is
polling queue for completion (mlx5hws_bwc_queue_poll) in header
file, so that it will be used by complex matcher code.

Signed-off-by: Yevgeny Kliteynik <kliteyn@nvidia.com>
Reviewed-by: Vlad Dogaru <vdogaru@nvidia.com>
Reviewed-by: Mark Bloch <mbloch@nvidia.com>
Signed-off-by: Tariq Toukan <tariqt@nvidia.com>
Link: https://patch.msgid.link/1746992290-568936-4-git-send-email-tariqt@nvidia.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Acked-by: Thomas Bogendoerfer <tbogendoerfer@suse.de>
---
 drivers/net/ethernet/mellanox/mlx5/core/steering/hws/bwc.c |   29 +++++++------
 drivers/net/ethernet/mellanox/mlx5/core/steering/hws/bwc.h |    5 ++
 2 files changed, 21 insertions(+), 13 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/bwc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/bwc.c
@@ -223,10 +223,10 @@ int mlx5hws_bwc_matcher_destroy(struct m
 	return 0;
 }
 
-static int hws_bwc_queue_poll(struct mlx5hws_context *ctx,
-			      u16 queue_id,
-			      u32 *pending_rules,
-			      bool drain)
+int mlx5hws_bwc_queue_poll(struct mlx5hws_context *ctx,
+			   u16 queue_id,
+			   u32 *pending_rules,
+			   bool drain)
 {
 	unsigned long timeout = jiffies +
 				secs_to_jiffies(MLX5HWS_BWC_POLLING_TIMEOUT);
@@ -361,7 +361,8 @@ hws_bwc_rule_destroy_hws_sync(struct mlx
 	if (unlikely(ret))
 		return ret;
 
-	ret = hws_bwc_queue_poll(ctx, rule_attr->queue_id, &expected_completions, true);
+	ret = mlx5hws_bwc_queue_poll(ctx, rule_attr->queue_id,
+				     &expected_completions, true);
 	if (unlikely(ret))
 		return ret;
 
@@ -442,9 +443,8 @@ hws_bwc_rule_create_sync(struct mlx5hws_
 	if (unlikely(ret))
 		return ret;
 
-	ret = hws_bwc_queue_poll(ctx, rule_attr->queue_id, &expected_completions, true);
-
-	return ret;
+	return mlx5hws_bwc_queue_poll(ctx, rule_attr->queue_id,
+				      &expected_completions, true);
 }
 
 static int
@@ -465,7 +465,8 @@ hws_bwc_rule_update_sync(struct mlx5hws_
 	if (unlikely(ret))
 		return ret;
 
-	ret = hws_bwc_queue_poll(ctx, rule_attr->queue_id, &expected_completions, true);
+	ret = mlx5hws_bwc_queue_poll(ctx, rule_attr->queue_id,
+				     &expected_completions, true);
 	if (unlikely(ret))
 		mlx5hws_err(ctx, "Failed updating BWC rule (%d)\n", ret);
 
@@ -651,8 +652,10 @@ static int hws_bwc_matcher_move_all_simp
 							    &bwc_matcher->rules[i]) ?
 					       NULL : list_next_entry(bwc_rules[i], list_node);
 
-				ret = hws_bwc_queue_poll(ctx, rule_attr.queue_id,
-							 &pending_rules[i], false);
+				ret = mlx5hws_bwc_queue_poll(ctx,
+							     rule_attr.queue_id,
+							     &pending_rules[i],
+							     false);
 				if (unlikely(ret)) {
 					mlx5hws_err(ctx,
 						    "Moving BWC rule failed during rehash (%d)\n",
@@ -669,8 +672,8 @@ static int hws_bwc_matcher_move_all_simp
 			u16 queue_id = mlx5hws_bwc_get_queue_id(ctx, i);
 
 			mlx5hws_send_engine_flush_queue(&ctx->send_queue[queue_id]);
-			ret = hws_bwc_queue_poll(ctx, queue_id,
-						 &pending_rules[i], true);
+			ret = mlx5hws_bwc_queue_poll(ctx, queue_id,
+						     &pending_rules[i], true);
 			if (unlikely(ret)) {
 				mlx5hws_err(ctx,
 					    "Moving BWC rule failed during rehash (%d)\n", ret);
--- a/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/bwc.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/bwc.h
@@ -64,6 +64,11 @@ void mlx5hws_bwc_rule_fill_attr(struct m
 				u32 flow_source,
 				struct mlx5hws_rule_attr *rule_attr);
 
+int mlx5hws_bwc_queue_poll(struct mlx5hws_context *ctx,
+			   u16 queue_id,
+			   u32 *pending_rules,
+			   bool drain);
+
 static inline u16 mlx5hws_bwc_queues(struct mlx5hws_context *ctx)
 {
 	/* Besides the control queue, half of the queues are
