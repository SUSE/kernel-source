From 98b6fa62c84f2e129161e976a5b9b3cb4ccd117b Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Wed, 27 Aug 2025 15:27:30 -0600
Subject: [PATCH] io_uring/kbuf: always use READ_ONCE() to read ring provided
 buffer lengths
Git-commit: 98b6fa62c84f2e129161e976a5b9b3cb4ccd117b
Patch-mainline: v6.17-rc4
References: CVE-2025-39816 bsc#1249906

Since the buffers are mapped from userspace, it is prudent to use
READ_ONCE() to read the value into a local variable, and use that for
any other actions taken. Having a stable read of the buffer length
avoids worrying about it changing after checking, or being read multiple
times.

Similarly, the buffer may well change in between it being picked and
being committed. Ensure the looping for incremental ring buffer commit
stops if it hits a zero sized buffer, as no further progress can be made
at that point.

Fixes: ae98dbf43d75 ("io_uring/kbuf: add support for incremental buffer consumption")
Link: https://lore.kernel.org/io-uring/tencent_000C02641F6250C856D0C26228DE29A3D30A@qq.com/
Reported-by: Qingyue Zhang <chunzhennn@qq.com>
Reported-by: Suoxing Zhang <aftern00n@qq.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Gabriel Krisman Bertazi <krisman@suse.de>
---
 io_uring/kbuf.c |    8 +++++---
 io_uring/kbuf.h |   12 ++++++++----
 2 files changed, 13 insertions(+), 7 deletions(-)

--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -140,6 +140,7 @@ static void __user *io_ring_buffer_selec
 	__u16 tail, head = bl->head;
 	struct io_uring_buf *buf;
 	void __user *ret;
+	u32 buf_len;
 
 	tail = smp_load_acquire(&br->tail);
 	if (unlikely(tail == head))
@@ -149,8 +150,9 @@ static void __user *io_ring_buffer_selec
 		req->flags |= REQ_F_BL_EMPTY;
 
 	buf = io_ring_head_to_buf(br, head, bl->mask);
-	if (*len == 0 || *len > buf->len)
-		*len = buf->len;
+	buf_len = READ_ONCE(buf->len);
+	if (*len == 0 || *len > buf_len)
+		*len = buf_len;
 	req->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;
 	req->buf_list = bl;
 	req->buf_index = buf->bid;
@@ -257,7 +259,7 @@ static int io_ring_buffers_peek(struct i
 
 	req->buf_index = buf->bid;
 	do {
-		u32 len = buf->len;
+		u32 len = READ_ONCE(buf->len);
 
 		/* truncate end piece, if needed, for non partial buffers */
 		if (len > arg->max_len) {
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -140,15 +140,19 @@ static inline bool io_kbuf_commit(struct
 
 	if (bl->flags & IOBL_INC) {
 		struct io_uring_buf *buf;
+		u32 buf_len;
 
 		buf = io_ring_head_to_buf(bl->buf_ring, bl->head, bl->mask);
-		if (WARN_ON_ONCE(len > buf->len))
-			len = buf->len;
-		buf->len -= len;
-		if (buf->len) {
+		buf_len = READ_ONCE(buf->len);
+		if (WARN_ON_ONCE(len > buf_len))
+			len = buf_len;
+		buf_len -= len;
+		if (buf_len) {
 			buf->addr += len;
+			buf->len = buf_len;
 			return false;
 		}
+		buf->len = 0;
 	}
 
 	bl->head += nr;
