From d4c209ccf12562232e5624882cc4581585bdb329 Mon Sep 17 00:00:00 2001
From: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date: Fri, 4 Aug 2023 14:40:15 +0200
Subject: [PATCH] signal: Update the comment ptrace_stop().

References: SLE Realtime Extension
Git-commit: 26339c5bc6d379408218bf53598821f1502bdeea
Patch-mainline: Queued in subsystem maintainer repository (v6.5-rc4-rt2)
Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/rt/linux-rt-devel.git/

Update the comment to the version that has been posted on the list.

Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Link: https://lore.kernel.org/r/20230803100932.325870-2-bigeasy@linutronix.de
Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 kernel/signal.c | 8 ++++++--
 1 file changed, 6 insertions(+), 2 deletions(-)

diff --git a/kernel/signal.c b/kernel/signal.c
index 6d7957487169..7ce09b42fa2e 100644
--- a/kernel/signal.c
+++ b/kernel/signal.c
@@ -2333,8 +2333,12 @@ static int ptrace_stop(int exit_code, int why, unsigned long message,
 	 * between unlock and schedule() and so improving the performance since
 	 * the ptracer has no reason to sleep.
 	 *
-	 * This optimisation is not doable on PREEMPT_RT due to the spinlock_t
-	 * within the preempt-disable section.
+	 * On PREEMPT_RT locking tasklist_lock does not disable preemption.
+	 * Therefore the task can be preempted (after
+	 * do_notify_parent_cldstop()) before unlocking tasklist_lock so there
+	 * is no benefit in doing this. The optimisation is harmful on
+	 * PEEMPT_RT because the spinlock_t (in cgroup_enter_frozen()) must not
+	 * be acquired with disabled preemption.
 	 */
 	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
 		preempt_disable();
