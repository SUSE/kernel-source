From 9f902ac73f9c2b03a9a7a67fadf40d4327deb2d1 Mon Sep 17 00:00:00 2001
From: Gabriel Krisman Bertazi <krisman@suse.de>
Date: Fri, 28 Jul 2023 14:21:30 -0400
Subject: [PATCH] io_uring: Acquire completion_lock around io_get_deferred_req
Patch-mainline: Never, SLE-specific patch. Upstream was fixed in refactoring
References: bsc#1213272 CVE-2023-21400

In order to prevent concurrent access to ->defer_list, acquire the
ctx->completion_lock acround touching the defer-list also in the iopoll
path.

This is similar to the stable-only commit fb348857e7b6 ("io_uring: ensure
IOPOLL locks around deferred work"), applied against 5.15.y.  We don't
have an equivalent for <5.15 (i.e. 5.10 lts), because of the forklift
done to stable.  This approach is simpler than that patch while
protecting ->defer_List the same way.

Signed-off-by: Gabriel Krisman Bertazi <krisman@suse.de>
---
 fs/io_uring.c | 17 +++++++++++++----
 1 file changed, 13 insertions(+), 4 deletions(-)

diff --git a/fs/io_uring.c b/fs/io_uring.c
index 150db773a664..021d10a672f0 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -494,18 +494,22 @@ static void __io_commit_cqring(struct io_ring_ctx *ctx)
 	}
 }
 
-static void io_commit_cqring(struct io_ring_ctx *ctx)
+static void __io_commit_cqring_flush(struct io_ring_ctx *ctx)
 {
 	struct io_kiocb *req;
 
-	__io_commit_cqring(ctx);
-
 	while ((req = io_get_deferred_req(ctx)) != NULL) {
 		req->flags |= REQ_F_IO_DRAINED;
 		queue_work(ctx->sqo_wq, &req->work);
 	}
 }
 
+static void io_commit_cqring(struct io_ring_ctx *ctx)
+{
+	__io_commit_cqring(ctx);
+	__io_commit_cqring_flush(ctx);
+}
+
 static struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)
 {
 	struct io_cq_ring *ring = ctx->cq_ring;
@@ -734,6 +738,7 @@ static void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,
 	void *reqs[IO_IOPOLL_BATCH];
 	struct io_kiocb *req;
 	int to_free;
+	unsigned long flags;
 
 	to_free = 0;
 	while (!list_empty(done)) {
@@ -760,7 +765,11 @@ static void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,
 		}
 	}
 
-	io_commit_cqring(ctx);
+	__io_commit_cqring(ctx);
+	spin_lock_irqsave(&ctx->completion_lock, flags);
+	__io_commit_cqring_flush(ctx);
+	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+
 	io_free_req_many(ctx, reqs, &to_free);
 }
 
-- 
2.41.0

