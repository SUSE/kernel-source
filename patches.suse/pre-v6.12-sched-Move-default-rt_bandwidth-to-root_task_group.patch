From: =?utf-8?b?TWljaGFsIEtvdXRuw70gPG1rb3V0bnlAc3VzZS5jb20+?=
Date: Wed, 27 Nov 2024 22:34:17 +0100
Subject: (pre v6.12) sched: Move default rt_bandwidth to root_task_group
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Patch-mainline: Never, we don't have fair_server
References: jsc#PED-11945

Global RT throttling (sysctl) accounts both for RT tasks and DL (deadline) tasks.
def_rt_bandwidth is used when !CONFIG_RT_GROUP_SCHED as the storage.

However, this storage is only truly needed with !CONFIG_CGROUP_SCHED,
therefore with CONFIG_CGROUP_SCHED we can use the rt_bandwidth storage
embedded in root_task_group. The benefit is the accessor functions don't
need to be split on CONFIG_RT_GROUP_SCHED and common implementation can
be used (added in a separate patch).

Global (sysctl) knobs hence write through to root_task_group, i.e. we
lose the option to configure different global and root task_group RT
limit. (The difference can serve as exclusive room for DL tasks.)

This should not compile with !CONFIG_CGROUP_SCHED (no root_task_group).

This patch is replacement of commit
5f6bd380c7bdb ("sched/rt: Remove default bandwidth control") v6.12-rc1~120^2~50
to preserve global throttling without DL fair_server.

Signed-off-by: Michal Koutn√Ω <mkoutny@suse.com>
---
 kernel/sched/core.c  |  6 +-----
 kernel/sched/rt.c    | 22 ++++++++++------------
 kernel/sched/sched.h |  3 +--
 3 files changed, 12 insertions(+), 19 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 49abae2f11a2b..7bcdeac409003 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -10044,16 +10044,12 @@ void __init sched_init(void)
 #endif /* CONFIG_RT_GROUP_SCHED */
 	}
 
-	init_rt_bandwidth(&def_rt_bandwidth, global_rt_period(), global_rt_runtime());
-
 #ifdef CONFIG_SMP
 	init_defrootdomain();
 #endif
 
-#ifdef CONFIG_RT_GROUP_SCHED
 	init_rt_bandwidth(&root_task_group.rt_bandwidth,
 			global_rt_period(), global_rt_runtime());
-#endif /* CONFIG_RT_GROUP_SCHED */
 
 #ifdef CONFIG_CGROUP_SCHED
 	task_group_cache = KMEM_CACHE(task_group, 0);
@@ -10100,7 +10096,7 @@ void __init sched_init(void)
 		init_tg_cfs_entry(&root_task_group, &rq->cfs, NULL, i, NULL);
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
-		rq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;
+		rq->rt.rt_runtime = root_task_group.rt_bandwidth.rt_runtime;
 #ifdef CONFIG_RT_GROUP_SCHED
 		init_tg_rt_entry(&root_task_group, &rq->rt, NULL, i, NULL);
 #endif
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 84f96824eb27a..427bd1a357991 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -10,8 +10,6 @@ static const u64 max_rt_runtime = MAX_BW;
 
 static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun);
 
-struct rt_bandwidth def_rt_bandwidth;
-
 /*
  * period over which we measure -rt task CPU usage in us.
  * default: 1s
@@ -258,7 +256,7 @@ int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)
 		goto err;
 
 	init_rt_bandwidth(&tg->rt_bandwidth,
-			ktime_to_ns(def_rt_bandwidth.rt_period), 0);
+			ktime_to_ns(root_task_group.rt_bandwidth.rt_period), 0);
 
 	for_each_possible_cpu(i) {
 		rt_rq = kzalloc_node(sizeof(struct rt_rq),
@@ -609,7 +607,7 @@ static inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)
 
 static inline u64 sched_rt_period(struct rt_rq *rt_rq)
 {
-	return ktime_to_ns(def_rt_bandwidth.rt_period);
+	return ktime_to_ns(root_task_group.rt_bandwidth.rt_period);
 }
 
 typedef struct rt_rq *rt_rq_iter_t;
@@ -659,7 +657,7 @@ struct rt_rq *sched_rt_period_rt_rq(struct rt_bandwidth *rt_b, int cpu)
 
 static inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)
 {
-	return &def_rt_bandwidth;
+	return &root_task_group.rt_bandwidth;
 }
 
 #endif /* CONFIG_RT_GROUP_SCHED */
@@ -1186,7 +1184,7 @@ dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)
 static void
 inc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)
 {
-	start_rt_bandwidth(&def_rt_bandwidth);
+	start_rt_bandwidth(&root_task_group.rt_bandwidth);
 }
 
 static inline
@@ -2915,7 +2913,7 @@ static int sched_rt_global_constraints(void)
 	unsigned long flags;
 	int i;
 
-	raw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);
+	raw_spin_lock_irqsave(&root_task_group.rt_bandwidth.rt_runtime_lock, flags);
 	for_each_possible_cpu(i) {
 		struct rt_rq *rt_rq = &cpu_rq(i)->rt;
 
@@ -2923,7 +2921,7 @@ static int sched_rt_global_constraints(void)
 		rt_rq->rt_runtime = global_rt_runtime();
 		raw_spin_unlock(&rt_rq->rt_runtime_lock);
 	}
-	raw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);
+	raw_spin_unlock_irqrestore(&root_task_group.rt_bandwidth.rt_runtime_lock, flags);
 
 	return 0;
 }
@@ -2946,10 +2944,10 @@ static void sched_rt_do_global(void)
 {
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);
-	def_rt_bandwidth.rt_runtime = global_rt_runtime();
-	def_rt_bandwidth.rt_period = ns_to_ktime(global_rt_period());
-	raw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);
+	raw_spin_lock_irqsave(&root_task_group.rt_bandwidth.rt_runtime_lock, flags);
+	root_task_group.rt_bandwidth.rt_runtime = global_rt_runtime();
+	root_task_group.rt_bandwidth.rt_period = ns_to_ktime(global_rt_period());
+	raw_spin_unlock_irqrestore(&root_task_group.rt_bandwidth.rt_runtime_lock, flags);
 }
 
 static int sched_rt_handler(struct ctl_table *table, int write, void *buffer,
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5c80cde66e17f..a3d21c2a7f9e8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -378,8 +378,8 @@ struct task_group {
 	struct sched_rt_entity	**rt_se;
 	struct rt_rq		**rt_rq;
 
-	struct rt_bandwidth	rt_bandwidth;
 #endif
+	struct rt_bandwidth	rt_bandwidth;
 
 	struct rcu_head		rcu;
 	struct list_head	list;
@@ -2425,7 +2425,6 @@ extern void reweight_task(struct task_struct *p, int prio);
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
-extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 extern bool sched_rt_bandwidth_account(struct rt_rq *rt_rq);
 

