From: =?utf-8?b?TWljaGFsIEtvdXRuw70gPG1rb3V0bnlAc3VzZS5jb20+?=
Date: Wed, 27 Nov 2024 22:34:17 +0100
Subject: (pre v6.12) sched: Move default rt_bandwidth to root_task_group
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Patch-mainline: v6.12-rc1
Git-commit: 5f6bd380c7bdbe10f7b4e8ddcceed60ce0714c6d (surrogate)
References: jsc#PED-11945

Global RT throttling (sysctl) accounts both for RT tasks and DL (deadline) tasks.
def_rt_bandwidth is used when !CONFIG_RT_GROUP_SCHED as the storage.

However, this storage is only truly needed with !CONFIG_CGROUP_SCHED,
therefore with CONFIG_CGROUP_SCHED we can use the rt_bandwidth storage
embedded in root_task_group. The benefit is the accessor functions don't
need to be split on CONFIG_RT_GROUP_SCHED and common implementation can
be used (added in a separate patch).

Global (sysctl) knobs hence write through to root_task_group, i.e. we
lose the option to configure different global and root task_group RT
limit. (The difference can serve as exclusive room for DL tasks.)

In order to work in !CONFIG_CGROUP_SCHED setups (such as zfcdump), we
synthesize a minimalistic root_task_group, so that higher patches can apply
without many changes.

This patch is replacement of commit
5f6bd380c7bdb ("sched/rt: Remove default bandwidth control") v6.12-rc1~120^2~50
to preserve global throttling without DL fair_server.

Signed-off-by: Michal Koutn√Ω <mkoutny@suse.com>
---
 include/linux/sched/autogroup.h |    2 --
 kernel/sched/core.c             |    9 ++++-----
 kernel/sched/rt.c               |   22 ++++++++++------------
 kernel/sched/sched.h            |    9 ++++++---
 4 files changed, 20 insertions(+), 22 deletions(-)

--- a/include/linux/sched/autogroup.h
+++ b/include/linux/sched/autogroup.h
@@ -25,8 +25,6 @@ static inline void sched_autogroup_exit(
 static inline void sched_autogroup_exit_task(struct task_struct *p) { }
 #endif
 
-#ifdef CONFIG_CGROUP_SCHED
 extern struct task_group root_task_group;
-#endif /* CONFIG_CGROUP_SCHED */
 
 #endif /* _LINUX_SCHED_AUTOGROUP_H */
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -10040,6 +10040,9 @@ LIST_HEAD(task_groups);
 
 /* Cacheline aligned slab cache for task_group */
 static struct kmem_cache *task_group_cache __read_mostly;
+#else
+/* Stub root_task_group for RT throttling */
+struct task_group root_task_group;
 #endif
 
 void __init sched_init(void)
@@ -10086,16 +10089,12 @@ void __init sched_init(void)
 #endif /* CONFIG_RT_GROUP_SCHED */
 	}
 
-	init_rt_bandwidth(&def_rt_bandwidth, global_rt_period(), global_rt_runtime());
-
 #ifdef CONFIG_SMP
 	init_defrootdomain();
 #endif
 
-#ifdef CONFIG_RT_GROUP_SCHED
 	init_rt_bandwidth(&root_task_group.rt_bandwidth,
 			global_rt_period(), global_rt_runtime());
-#endif /* CONFIG_RT_GROUP_SCHED */
 
 #ifdef CONFIG_CGROUP_SCHED
 	task_group_cache = KMEM_CACHE(task_group, 0);
@@ -10142,7 +10141,7 @@ void __init sched_init(void)
 		init_tg_cfs_entry(&root_task_group, &rq->cfs, NULL, i, NULL);
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
-		rq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;
+		rq->rt.rt_runtime = root_task_group.rt_bandwidth.rt_runtime;
 #ifdef CONFIG_RT_GROUP_SCHED
 		init_tg_rt_entry(&root_task_group, &rq->rt, NULL, i, NULL);
 #endif
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -10,8 +10,6 @@ static const u64 max_rt_runtime = MAX_BW
 
 static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun);
 
-struct rt_bandwidth def_rt_bandwidth;
-
 /*
  * period over which we measure -rt task CPU usage in us.
  * default: 1s
@@ -255,7 +253,7 @@ int alloc_rt_sched_group(struct task_gro
 		goto err;
 
 	init_rt_bandwidth(&tg->rt_bandwidth,
-			ktime_to_ns(def_rt_bandwidth.rt_period), 0);
+			ktime_to_ns(root_task_group.rt_bandwidth.rt_period), 0);
 
 	for_each_possible_cpu(i) {
 		rt_rq = kzalloc_node(sizeof(struct rt_rq),
@@ -614,7 +612,7 @@ static inline u64 sched_rt_runtime(struc
 
 static inline u64 sched_rt_period(struct rt_rq *rt_rq)
 {
-	return ktime_to_ns(def_rt_bandwidth.rt_period);
+	return ktime_to_ns(root_task_group.rt_bandwidth.rt_period);
 }
 
 typedef struct rt_rq *rt_rq_iter_t;
@@ -664,7 +662,7 @@ struct rt_rq *sched_rt_period_rt_rq(stru
 
 static inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)
 {
-	return &def_rt_bandwidth;
+	return &root_task_group.rt_bandwidth;
 }
 
 #endif /* CONFIG_RT_GROUP_SCHED */
@@ -1185,7 +1183,7 @@ dec_rt_group(struct sched_rt_entity *rt_
 static void
 inc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)
 {
-	start_rt_bandwidth(&def_rt_bandwidth);
+	start_rt_bandwidth(&root_task_group.rt_bandwidth);
 }
 
 static inline
@@ -2916,7 +2914,7 @@ static int sched_rt_global_constraints(v
 	unsigned long flags;
 	int i;
 
-	raw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);
+	raw_spin_lock_irqsave(&root_task_group.rt_bandwidth.rt_runtime_lock, flags);
 	for_each_possible_cpu(i) {
 		struct rt_rq *rt_rq = &cpu_rq(i)->rt;
 
@@ -2924,7 +2922,7 @@ static int sched_rt_global_constraints(v
 		rt_rq->rt_runtime = global_rt_runtime();
 		raw_spin_unlock(&rt_rq->rt_runtime_lock);
 	}
-	raw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);
+	raw_spin_unlock_irqrestore(&root_task_group.rt_bandwidth.rt_runtime_lock, flags);
 
 	return 0;
 }
@@ -2947,10 +2945,10 @@ static void sched_rt_do_global(void)
 {
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);
-	def_rt_bandwidth.rt_runtime = global_rt_runtime();
-	def_rt_bandwidth.rt_period = ns_to_ktime(global_rt_period());
-	raw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);
+	raw_spin_lock_irqsave(&root_task_group.rt_bandwidth.rt_runtime_lock, flags);
+	root_task_group.rt_bandwidth.rt_runtime = global_rt_runtime();
+	root_task_group.rt_bandwidth.rt_period = ns_to_ktime(global_rt_period());
+	raw_spin_unlock_irqrestore(&root_task_group.rt_bandwidth.rt_runtime_lock, flags);
 }
 
 static int sched_rt_handler(struct ctl_table *table, int write, void *buffer,
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -378,8 +378,8 @@ struct task_group {
 	struct sched_rt_entity	**rt_se;
 	struct rt_rq		**rt_rq;
 
-	struct rt_bandwidth	rt_bandwidth;
 #endif
+	struct rt_bandwidth	rt_bandwidth;
 
 	struct rcu_head		rcu;
 	struct list_head	list;
@@ -497,6 +497,9 @@ static inline void set_task_rq_fair(stru
 #else /* CONFIG_CGROUP_SCHED */
 
 struct cfs_bandwidth { };
+struct task_group {
+	struct rt_bandwidth	rt_bandwidth;
+};
 static inline bool cfs_task_bw_constrained(struct task_struct *p) { return false; }
 
 #endif	/* CONFIG_CGROUP_SCHED */
@@ -691,8 +694,9 @@ struct rt_rq {
 	unsigned int		rt_nr_boosted;
 
 	struct rq		*rq;
-	struct task_group	*tg;
 #endif
+	/* Used also with !CONFIG_CGROUPS */
+	struct task_group	*tg;
 };
 
 static inline bool rt_rq_is_runnable(struct rt_rq *rt_rq)
@@ -2426,7 +2430,6 @@ extern void reweight_task(struct task_st
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
-extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 extern bool sched_rt_bandwidth_account(struct rt_rq *rt_rq);
 
