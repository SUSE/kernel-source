From: Suren Baghdasaryan <surenb@google.com>
Date: Fri, 4 Aug 2023 08:27:21 -0700
Subject: mm: replace mmap with vma write lock assertions when operating on a
 vma
Git-commit: e727bfd5e73a35ecbc4a01a15c659b9fafaa97c0
Patch-mainline: v6.6-rc1
References: bsc#1219558

Vma write lock assertion always includes mmap write lock assertion and
additional vma lock checks when per-VMA locks are enabled. Replace
weaker mmap_assert_write_locked() assertions with stronger
vma_assert_write_locked() ones when we are operating on a vma which
is expected to be locked.

Link: https://lkml.kernel.org/r/20230804152724.3090321-4-surenb@google.com
Suggested-by: Jann Horn <jannh@google.com>
Signed-off-by: Suren Baghdasaryan <surenb@google.com>
Reviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>
Cc: Linus Torvalds <torvalds@linuxfoundation.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
---
 mm/hugetlb.c |    2 +-
 mm/memory.c  |    2 +-
 2 files changed, 2 insertions(+), 2 deletions(-)

--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -5058,7 +5058,7 @@ int copy_hugetlb_page_range(struct mm_st
 					src_vma->vm_start,
 					src_vma->vm_end);
 		mmu_notifier_invalidate_range_start(&range);
-		mmap_assert_write_locked(src);
+		vma_assert_write_locked(src_vma);
 		raw_write_seqcount_begin(&src->write_protect_seq);
 	} else {
 		/*
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1293,7 +1293,7 @@ copy_page_range(struct vm_area_struct *d
 		 * Use the raw variant of the seqcount_t write API to avoid
 		 * lockdep complaining about preemptibility.
 		 */
-		mmap_assert_write_locked(src_mm);
+		vma_assert_write_locked(src_vma);
 		raw_write_seqcount_begin(&src_mm->write_protect_seq);
 	}
 
