#!/usr/bin/python3
import sys, re, os, argparse, datetime, bugzilla, subprocess, multiprocessing, json, time, requests
import git_sort.pygit2_wrapper as git
from bugzilla.utils import get_bugzilla_api, check_being_logged_in, get_exportpatch_string, get_insert_string, make_url, get_score, handle_email, TIME_FORMAT_XML, TIME_FORMAT_REST
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import reduce

# dashboard kss script - is based on pygit2, python-bugzilla (our in-tree patched copy) and requests libraries
# for now this script should be kept Python 3.6 compatible (SLE15-SP7)

# the only external programs that might be invoked are:
#   suse-get-maintainers (--suse-get-maintainers, -2)
#   less (--pager, -Z)
#   ./scripts/check-kernel-fix (--check-kernel-fix, -0)
#   exportpatch and ./scripts/git_sort/series_insert (--exportpatch, -1)

# other requirements
#   https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git ($LINUX_GIT)
#   https://git.kernel.org/pub/scm/linux/security/vulns.git ($VULNS_GIT)
#   ~/.bugzillarc (with user token)
#   suse-get-maintainers
#   patchtools (for the exportpatch)

CVE_PATTERN = re.compile(r"CVE-[0-9]{4}-[0-9]{4,}")
VECTOR_PATTERN = re.compile(r"CVSSv3.1:SUSE:CVE-[0-9]{4}-[0-9]{4,}:[0-9]{1,}.[0-9]:\(AV:(.)\/AC:(.)\/PR:(.)\/UI:(.)\/S:(.)\/C:(.)\/I:(.)\/A:(.)\)")
ZDI_PATTERN = re.compile(r"ZDI-[0-9]{2}-[0-9]{3,}")
FIXES_PATTERN = re.compile(r"Fixes: ([0-9a-f]{12,})")
PATCH_PATTERN = re.compile(r"\+\+\+ b/([^ ]{1,})\n")
COMMENT_BANLIST = [ 'swamp@suse.de', 'bwiedemann+obsbugzillabot@suse.com', 'maint-coord+maintenance-robot@suse.de', 'smash_bz@suse.de' ]
GIT_ROOT = '1da177e4c3f41524e886b7f1b8a0c1fc7321cac2'
SMASH_URL = 'https://smash.suse.de/api/issues/'
REPO_EXPIRATION = 3600 * 8

T_RED = "\033[01;31m"
T_GREEN = "\033[01;32m"
T_YELLOW = "\033[01;33m"
T_BLUE = "\033[01;34m"
T_PURPLE = "\033[01;35m"
T_END = "\033[0m"
show_colors = os.isatty(sys.stdout.fileno())
today = datetime.date.today()
sgm_present = False
time_format = None
smash_token = None

def get_smash_token():
    homedir = os.getenv('HOME', None)
    if not homedir:
        sys.exit(2)
    smash_file = homedir + os.sep + '.smashrc'
    token = ''
    try:
        with open(smash_file, 'r') as f:
            token = f.read().strip()
    except:
        print(f"Cannot read: {smash_file}")
    if token:
        return token
    return None

def fetch_smash_data(token, issue):
    headers = { "Authorization": f"Token {token}" }
    try:
        response = requests.get(SMASH_URL + "/" + str(issue), headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err}", file=sys.stderr)
        if response:
            print(f"Response content: {response.text}", file=sys.stderr)
    except requests.exceptions.ConnectionError as conn_err:
        print(f"Connection error occurred: {conn_err}", file=sys.stderr)
    except requests.exceptions.Timeout as timeout_err:
        print(f"Timeout error occurred: {timeout_err}", file=sys.stderr)
    except requests.exceptions.RequestException as req_err:
        print(f"An error occurred: {req_err}", file=sys.stderr)
    except json.JSONDecodeError:
        print(f"Failed to decode JSON from response: {response.text if response is not None else 'No response'}", file=sys.stderr)
    return None

def get_issue_from_smash_url(url):
    return url.strip('/').split('/')[-1]

def show_smash_data(url):
    issue = get_issue_from_smash_url(url)
    raw_data = fetch_smash_data(smash_token, issue)
    aps = raw_data.get('affected_packages', [])
    for package in aps:
        if package.get('relation', '') == 'Affected':
            codestream = package.get("codestream", "")
            if codestream:
                yield f'{"":>46}{color_format(T_PURPLE, codestream + "/" + package.get("package", ""))}\n'
            product = package.get("product", "")
            if product:
                yield f'{"":>46}{color_format(T_YELLOW, product.get("cpe", "") + "/" + package.get("package", ""))}\n'

def format_time(t):
    return datetime.datetime.strptime(str(t), time_format)

def color_format(color, msg):
    if show_colors and msg:
        return "{}{}{}".format(color, msg, T_END)
    return msg

# a decorator for error handling, so we don't have to repeat the same error handling over and over again
class ExitOnException:
    def __init__(self, msg):
        self.msg = msg

    def _override(self, *args):
        return "failed to "

    def __call__(self, f):
        def wrapper(*args, **kwargs):
            try:
                return f(*args, **kwargs)
            except Exception as e:
                print(f'{color_format(T_RED, self._override(*args) + self.msg)}: {e}', file=sys.stderr)
                sys.exit(1)
        return wrapper

class ExitOnExceptionHandle(ExitOnException):
    def _override(self, *args):
        return args[0]

def check_envvar(var):
    result = os.environ.get(var)
    if not result:
        print("Please set {} environment variable!\n\nThe script requires a clone of https://git.kernel.org/pub/scm/linux/security/vulns.git in VULNS_GIT\n"\
              "and a clone of https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git in LINUX_GIT ".format(var), file=sys.stderr)
        sys.exit(1)
    return result

def make_zdi_url(zdi_id):
    return f'https://www.zerodayinitiative.com/advisories/{zdi_id}/'

def get_zdi(s):
    m = re.search(ZDI_PATTERN, s)
    return m.group(0) if m else ''

def get_cve(s):
    m = re.search(CVE_PATTERN, s)
    return m.group(0) if m else ''

# a vestige from the times when the bugzilla didn't store the deadline field, it's still used in case the deadline field is missing, but perhaps could be taken out
def make_deadline(score):
    base_score = 0
    try:
        base_score = int(score.split('.')[0])
    except:
        pass
    if base_score >= 7:
        return 30
    if base_score >= 4:
        return 90
    return None

def validate_characters(sha):
    for c in sha:
        if c not in "0123456789abcdefABCDEF":
            return True
    return False

def validate_hashes(cves):
    for k, cve in cves.items():
        for v in cve:
            if v and (len(v) != 40 or validate_characters(v)):
                print(color_format(T_RED, f'{v} is not a valid hash for a cve {k}'), file=sys.stderr)
                cves[k] = [e for e in cves[k] if e != v]

def check_stale_data(path_to_repo):
    path = f'{path_to_repo}{os.sep}.git/refs/remotes/origin/master'
    delta = time.time() - os.path.getmtime(path)
    return delta > REPO_EXPIRATION

# if kernel CNA decides to change/reinterpret the layout of the vulnerability DB again, this needs to be reworked
def fetch_cves(cves, branch):
    path_to_repo = check_envvar('VULNS_GIT')
    repo = git.Repository(path_to_repo)
    if (check_stale_data(path_to_repo)):
        repo.remotes["origin"].fetch()
    tree_index = git.Index()
    try:
        tree_index.read_tree(repo.revparse_single(branch).tree)
    except Exception as e:
        print(color_format(T_RED, f'branch {branch} probably does not exist: {e}'), file=sys.stderr)
        sys.exit(1)
    def path_contains_one_of(path, elements):
        for e in elements:
            if e in path:
                return True
        return False
    path_filtered_tree_data = [ t for t in tree_index if t.path.startswith('cve/') and t.path.endswith('.sha1') and path_contains_one_of(t.path, cves) ]
    published_data = [ (t.path, t.id) for t in path_filtered_tree_data if t.path.startswith('cve/published/') ]
    rejected_data = [ (t.path, t.id) for t in path_filtered_tree_data if t.path.startswith('cve/rejected/') ]
    path_filtered_tree_vulnerable_data = [ t for t in tree_index if t.path.startswith('cve/') and t.path.endswith('.vulnerable') and path_contains_one_of(t.path, cves) ]
    return ( { get_cve(t[0]): repo[t[1]].data.decode('ascii').rstrip().split() for t in published_data },
             { get_cve(t[0]): repo[t[1]].data.decode('ascii').rstrip().split() for t in rejected_data },
             { get_cve(t.path): repo[t.id].data.decode('ascii').rstrip().split() for t in path_filtered_tree_vulnerable_data })

def show_patch(p, h):
    yield f'--- Begin {h} ---\n'
    for l in p.splitlines():
        if l.startswith('-'):
            yield color_format(T_RED, l) + '\n'
        elif l.startswith('+'):
            yield color_format(T_GREEN, l) + '\n'
        else:
            yield l + '\n'
    yield f'--- End {h} ---\n'

def show_stats(linux_git, h):
    changes = color_format(T_PURPLE, f'{linux_git.pstats[h]["added"] + linux_git.pstats[h]["removed"]:>4}')
    added = color_format(T_GREEN, '{:>4}'.format(f'{linux_git.pstats[h]["added"]:+}'))
    removed = color_format(T_RED, '{:>4}'.format(f'{-linux_git.pstats[h]["removed"]:+}'))
    yield f'     changes: {changes}    ( {added} , {removed} )    files: {linux_git.pstats[h]["files"]}\n'

# implements CVSSv3.1 score breakdown for the -A option (based on https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator)
class ScoreVector:
    def __init__(self, vector):
        tav = { 'P': color_format(T_BLUE, 'Physical'), 'L': color_format(T_GREEN, 'Local   '),
                'A': color_format(T_YELLOW, 'Adjacent'), 'N': color_format(T_RED, 'Network ') }
        ttt = { 'N': color_format(T_RED, 'None'), 'H': color_format(T_GREEN, 'High'),
                'L': color_format(T_YELLOW, 'Low '), 'R': color_format(T_YELLOW, 'Required'),
                'U': color_format(T_YELLOW, 'Unchanged'), 'C': color_format(T_RED, 'Changed  ') }
        tim = { 'N': color_format(T_GREEN, 'None'), 'L': color_format(T_YELLOW, 'Low '),
                'H': color_format(T_RED, 'High') }
        self.m = re.search(VECTOR_PATTERN, vector)
        if self.m:
            self.vals= [
                ('Attack Vector', tav.get(self.m.group(1))),
                ('Attack Complexity', ttt.get(self.m.group(2))),
                ('Privileges Required', ttt.get(self.m.group(3))),
                ('User Interaction', ttt.get(self.m.group(4))),
                ('Scope', ttt.get(self.m.group(5))),
                ('Confidentiality Impact', tim.get(self.m.group(6))),
                ('Integrity Impact', tim.get(self.m.group(7))),
                ('Availability Impact', tim.get(self.m.group(8)))]

    def __str__(self):
        return ' | '.join("{}: {}{}".format(i[0], i[1], T_END) for i in self.vals)

def sha_or_nothing(repo, f):
    try:
        return repo.revparse_single(f).id
    except:
        return ''

# keeps not only the libgit2 repository instance, but also caches for its patches and patch statistics, so they don't have to be recalculated
class MainlineRepo:
    def __init__(self):
        path_to_repo = check_envvar('LINUX_GIT')
        self.repo = git.Repository(path_to_repo)
        if (check_stale_data(path_to_repo)):
            self.repo.remotes["origin"].fetch()
        self.patches = dict()
        self.pstats = dict()

    def get_all_fixes(self, h):
        try:
            msg = self.repo[h].raw_message.decode('utf8')
            return { str(sha_or_nothing(self.repo, f)) for f in re.findall(FIXES_PATTERN, msg) }
        except KeyError as e:
            print(color_format(T_RED, f'Missing {e}.  Please call `git -C {check_envvar("LINUX_GIT")} fetch --all` and retry.'), file=sys.stderr)

    def get_patch(self, h):
        if h not in self.patches:
            t0 = self.repo.revparse_single(h + "^")
            t1 = self.repo.revparse_single(h)
            self.patches[h] = self.repo.diff(t0, t1).patch
            self.pstats[h] = dict()
            self.pstats[h]['added'] = 0
            self.pstats[h]['removed'] = 0
            self.pstats[h]['files'] = 0
            for l in self.patches[h].splitlines():
                if l.startswith('-') and not l.startswith('---'):
                    self.pstats[h]['removed'] += 1
                elif l.startswith('+') and not l.startswith('+++'):
                    self.pstats[h]['added'] += 1
                elif l.startswith('diff '):
                    self.pstats[h]['files'] += 1
        return self.patches[h]

    def get_all_paths(self, h):
        return { p.lstrip() for p in re.findall(PATCH_PATTERN, self.get_patch(h)) }

class BugData:
    def __init__(self, bug):
        self.is_rejected = False
        self.data = bug
        self.cve = get_cve(self.data.summary)
        self.zdi = get_zdi(self.data.summary)
        self.shas = []
        self.breakers = []
        self.score = get_score(self.data.status_whiteboard)
        self.bz_deadline = datetime.datetime.strptime(self.data.deadline, "%Y-%m-%d").date() if hasattr(self.data, 'deadline') and self.data.deadline else None
        self.deadline = make_deadline(self.score)
        self.mtime = format_time(bug.last_change_time)
        self.btime = format_time(bug.creation_time)
        self.age = (today - self.btime.date()).days
        if self.bz_deadline:
            self.overdue = (today - self.bz_deadline).days
        else:
            self.overdue = None if not self.deadline else - (self.deadline - self.age)
        self.paths = None
        self.vector = None
        self.comments = []
        self.smash_comments = []
        self.matched_comments = []
        self.changes = 0
        self.history = None
        self.embargoed = 'EMBARGOED' in self.data.summary
        self.sgm_data = {}

    def update_vulns(self, published, rejected, breakers):
        self.breakers = list(set(breakers.get(self.cve, '')))
        self.shas = published.get(self.cve, '')
        if not self.shas:
            self.shas = rejected.get(self.cve, '')
            if self.shas:
                self.is_rejected = True

    def grep_paths(self, regex, linux_git):
        for s in self.shas:
            self.paths = linux_git.get_all_paths(s)
            if [ p for p in self.paths if re.search(regex, p) ]:
                return True
        return False

    def get_stats(self, linux_git):
        for s in self.shas:
            linux_git.get_patch(s)
            self.changes += (linux_git.pstats[s]['added'] + linux_git.pstats[s]['removed'])
        return self.changes

    def grep_patch(self, regex, linux_git):
        for s in self.shas:
            patch = linux_git.get_patch(s)
            if re.search(regex, patch):
                return True
        return False

    def store_comments(self, raw_comments):
        self.comments = [ { 'no': rc['count'], 'time': format_time(rc['time']), 'text': rc['text'], 'author': rc['creator'] } for rc in raw_comments
                          if rc['creator'] not in COMMENT_BANLIST ]
        self.smash_comments = [ { 'no': rc['count'], 'time': format_time(rc['time']), 'text': rc['text'], 'author': rc['creator'] } for rc in raw_comments
                          if rc['creator'] == 'smash_bz@suse.de' ]
        self.mtime = self.comments[-1]['time'] if self.comments else self.btime;
        return self

    def __show_comment_header(self, no, time, author):
        yield f'     [{no:>3}] at {time} by <{author + ">":<51} https://bugzilla.suse.com/show_bug.cgi?id={self.data.id}#c{no}\n'

    def show_last_comment_header(self):
        if self.comments:
            yield from self.__show_comment_header(self.comments[-1]['no'], self.comments[-1]['time'], self.comments[-1]['author'])
        else:
            yield color_format(T_RED, "     no comment\n")

    def show_comments_short(self):
        for c in sorted(self.comments, key=lambda x: x['no']):
            yield from self.__show_comment_header(c["no"], c["time"], c["author"])

    def __show_comments(self, comments):
        for c in sorted(comments, key=lambda x: x['no']):
            yield f'--- At {color_format(T_PURPLE, c["time"])} comment {color_format(T_GREEN, "#{}".format(c["no"]))} by <{color_format(T_YELLOW, c["author"])}> ---\n{c["text"]}\n\n---'\
                  f'Bugzilla url https://bugzilla.suse.com/show_bug.cgi?id={self.data.id}#c{c["no"]} ---\n\n\n'

    def show_comments_long(self):
        yield from self.__show_comments(self.comments)

    def show_comments_smash(self):
        yield from self.__show_comments(self.smash_comments)

    def show_history(self):
        space_outer = '\n' + ' ' * 62
        space_inner = space_outer + ' ' * 25
        for h in sorted(self.history, key=lambda x: x['when']):
            fra = (space_outer).join(
                [ f"{color_format(T_PURPLE, ch['field_name']):<{36 if show_colors else 24}} {color_format(T_RED, '- ' + ch['removed'])}{space_inner}{color_format(T_GREEN, '+ ' + ch['added'])}"
                  for ch in h['changes'] ])
            yield '{:>{}} ; {} : {}\n'.format(color_format(T_YELLOW, h['who']), 49 if show_colors else 37,  format_time(h['when']), fra)

    def store_history(self, raw_history):
        self.history = raw_history

    def show_sgm_data(self):
        subsystem_str = color_format(T_PURPLE, f"{self.sgm_data['subsystem']:<30}")
        maintainers_str = color_format(T_YELLOW, ', '.join(self.sgm_data['emails']))
        return f"{'':>5}Subsystem: {subsystem_str}Maintainers: {maintainers_str}\n"

    def grep_subsystem(self, regex, linux_git):
        if not self.sgm_data:
            return False
        return re.search(regex, self.sgm_data['subsystem'])

    def grep_maintainers(self, regex, linux_git):
        if self.sgm_data:
            for e in self.sgm_data['emails']:
                if re.search(regex, e):
                    return True
        return False

    def already_dispatched(self):
        for c in self.comments:
            if 'ACTION NEEDED' in c['text'] or 'NO CODESTREAM AFFECTED' in c['text'] or 'No codestream affected' in c['text']:
                return c['no']
        return 0

    def grep_comments(self, regex):
        for c in self.comments:
            m = re.search(regex, c['text'])
            if m:
                text = '--- At {0} comment #{1} by <{2}> ---\n'.format(c['time'], c['no'], c['author'])
                text += c['text'].replace(m.group(0), color_format(T_YELLOW, m.group(0)))
                text +='\n--- Bugzilla url https://bugzilla.suse.com/show_bug.cgi?id={}#c{} ---\n'.format(self.data.id, c['no'])
                self.matched_comments.append(text)
        return self.matched_comments

    def with_score_vector(self):
        self.vector = ScoreVector(self.data.status_whiteboard)
        return self

# a helper for additional tweaking of the main bugzilla query
def banlist_emails_in_bz_query(query, emails):
    for n, e in enumerate(emails, 1):
        query['query_format'] = 'advanced'
        query['email{}'.format(n)] = e
        query['emailassigned_to{}'.format(n)] = True
        query['emailtype{}'.format(n)] = 'notequals'

class BZApi:
    @ExitOnException("connect to bugzilla")
    def __init__(self, rest):
        self.bzapi = get_bugzilla_api(rest)
        if not check_being_logged_in(self.bzapi):
            sys.exit(1)

    @ExitOnException("fetch bug comments from bugzilla")
    def fetch_comments(self, bugs):
        bug_ids = [ b.data.id for b in bugs ]
        comments = self.bzapi.get_comments(bug_ids)
        for b in bugs:
            b.store_comments(comments['bugs'][str(b.data.id)]['comments'])

    @ExitOnException("fetch bug history from bugzilla")
    def fetch_history(self, bugs):
        bug_pairs = { b.data.id: b for b in bugs }
        history = self.bzapi.bugs_history_raw(list(bug_pairs.keys()))
        for h in history['bugs']:
            bug_pairs[h['id']].store_history(h['history'])

    # the main/intial query to the bugzilla, this requires careful tweaking
    @ExitOnException("fetch bugs from bugzilla")
    def fetch_bugs(self, email, bug_list = None, cve_list = None):
        kvargs = { 'status': ['NEW', 'IN_PROGRESS', 'CONFIRMED', 'REOPENED'],
                   'product': 'SUSE Security Incidents',
                   'component': 'Incidents',
                   'include_fields': ["id", "status", "summary", "status_whiteboard", "last_change_time", "creation_time", "assigned_to", "url", "deadline"] }
        # this is where our in-tree copy of python-bugzilla differs from upstream
        # we (SUSE bugzilla) use RESOLVED status, upstream uses CLOSED status instead
        # if this script should ever be packaged in RPM with python-bugzilla dependency, this needs to be worked around somewhat
        if bug_list:
            kvargs['bug_id'] = bug_list
            kvargs['status'].append('RESOLVED')
        elif cve_list:
            kvargs['alias'] = cve_list
            kvargs['status'].append('RESOLVED')
        elif email:
            kvargs['assigned_to'] = email

        query = self.bzapi.build_query(**kvargs)

        # python-bugzilla's build_query function is not powerful enough, so we have to tweak the result further manually
        if email == 'security-team@suse.de':
            query['short_desc'] = 'VUL- kernel'
            query['short_desc_type'] = 'allwordssubstr'

        if not email and not bug_list and not cve_list:
            banlist_emails_in_bz_query(query, ['cve-kpm@suse.de', 'security-team@suse.de', 'kernel-bugs@suse.de'])
            query['short_desc'] = 'VUL- kernel'
            query['short_desc_type'] = 'allwordssubstr'

        return self.bzapi.query(query)

# the main display function, a lot of spaghetti code in here
# a better solution would be to have __str__ function for every component of BugData which would reduce complexity somewhat
def show_bug(n, b, linux_git, args):
    color = T_RED
    fixes = []
    paths = []
    if b.shas:
        for s in b.shas:
            fixes.extend(linux_git.get_all_fixes(s))
        if fixes and b.score and b.shas:
            color = T_GREEN
        if args.files:
            if b.paths:
                paths = b.paths
            else:
                for s in b.shas:
                    paths.extend(linux_git.get_all_paths(s))
        if args.short:
            fixes = []
    if not args.short:
        yield f'{n:>3}; '
    middle = b.data.assigned_to if args.short and args.assigned_queue else ('' if not b.shas else color_format(color, b.shas[0]))
    padding_middle = 30 if args.short and args.assigned_queue else 40
    yield "{} ; {:<11} ; {:<14} ; {:>{}}".format(b.data.id, b.data.status, color_format(T_RED, b.cve) if b.is_rejected else b.cve, middle, padding_middle)
    yield " ; {:>4} ; {} ; {:>3} ; {}\n".format(b.score, b.btime if args.birth_time else b.mtime, b.age, color_format(T_RED, '[cve is rejected] ') + b.data.summary if b.is_rejected else b.data.summary)
    if len(b.shas) > 1:
        for s in b.shas[1:]:
            yield f'{"":>26}   {"additional fix":>14} : {color_format(color, s)}\n'
    for f in sorted(set(fixes)):
        in_vulns_str = ''
        if f in b.breakers:
            in_vulns_str = ', vulns'
        yield f'{"":>26}   {"breaker":>14} : {f} : from patch{in_vulns_str}\n'
    if not args.short:
        for f in sorted(set(b.breakers)):
            if f not in fixes:
                yield f'{"":>26}   {"breaker":>14} : {f} : from vulns\n'
    if not args.short and not args.comments_short and b.data.assigned_to not in ('kernel-bugs@suse.de', 'cve-kpm@suse.de'):
        yield from b.show_last_comment_header()
    if args.patch and b.shas:
        for s in b.shas:
            yield from show_patch(linux_git.get_patch(s), s)
    else:
        for p in sorted(set(paths)):
            yield f'{"":>46}{color_format(T_PURPLE, p)}\n'
    if args.breakers and fixes:
        for f in sorted(set(fixes) | set(b.breakers)):
            if f == GIT_ROOT:
                yield f'{color_format(T_RED, "Warning")}: the breaker {color_format(T_YELLOW, GIT_ROOT)} is the initial commit (Linux-2.6.12-rc2)\n'
            else:
                yield from show_patch(linux_git.get_patch(f), f)
    if args.stats_info and b.shas:
        for s in b.shas:
            linux_git.get_patch(s)
            yield from show_stats(linux_git, s)
    for c in b.matched_comments:
        yield c + '\n'
    if b.vector:
        yield f'> {b.vector} <\n\n'
    if b.history:
        yield from b.show_history()
    if args.deadlines:
        msg = "there is no deadline" if not b.overdue else f"the deadline is {-b.overdue} days from now"
        yield f'{color_format(T_RED, "FAIL") if b.overdue and b.overdue > 0 else color_format(T_GREEN, "OK")} : {msg}\n'
    if args.url and b.data.url:
        yield f'{"":>46}{color_format(T_PURPLE, b.data.url)}\n'
    if args.affected and smash_token and b.data.url:
        yield from show_smash_data(b.data.url)
    if args.comments_short:
        yield from b.show_comments_short()
    elif args.comments_long:
        yield from b.show_comments_long()
    elif args.comments_smash:
        yield from b.show_comments_smash()
    if not args.short:
        if args.suse_get_maintainers and b.sgm_data:
            yield b.show_sgm_data()
        if b.embargoed:
            yield f'{color_format(T_RED, "Warning: ")}{b.cve} is embargoed\n'
        if b.zdi:
            yield f'{color_format(T_RED, "Warning: ")}{make_zdi_url(b.zdi)}\n'
        if b.is_rejected:
            yield f'{color_format(T_RED, "Warning: ")}{b.cve} is rejected\n'
        if len(b.comments) > 5:
            yield f'{color_format(T_RED, "Warning: ")}{b.cve} has more than 5 human comments\n'
        if not args.deadlines and b.overdue and b.overdue > 0:
            yield f'{color_format(T_RED, "Warning: ")}{b.cve} is {b.overdue} days overdue\n'
        if not b.bz_deadline:
            yield f'{color_format(T_RED, "Warning: ")}{b.cve} has no deadline in bugzilla\n'
        if b.data.assigned_to == 'kernel-bugs@suse.de':
            c_no = b.already_dispatched()
            if c_no:
                yield f'{color_format(T_RED, "Warning: ")}{make_url(b.data.id)}#c{c_no} seems to have been already dispatched\n'

def chunks(a, n):
    for i in range(0, len(a), n):
        yield a[i:i+n]

def do_work(batch):
    ret = dict()
    for p in batch:
        with open(p, 'r', errors='ignore') as f:
            for l in f:
                if l.startswith('Alt-commit: ') or l.startswith('Git-commit: '):
                    sha = l.split()[1]
                    if not validate_characters(sha):
                        ret[sha] = p
    return ret

def get_hashdict(path):
    patches_to_process = [ f'{subdir}{os.sep}{f}' for subdir, _, files in os.walk(path) for f in files ]
    batches_to_process = [ b for b in chunks(patches_to_process, 256) ]
    result = []
    with ProcessPoolExecutor() as executor:
        futures = { executor.submit(do_work, batch): batch for batch in batches_to_process }
        for f in as_completed(futures):
            result.append(f.result())
    return reduce(lambda a, b: {**a, **b}, result)

def get_banlist(path):
    ret = set()
    with open(path, 'r') as f:
        for l in f:
            if len(l) > 40:
                sha = l[:40]
                if not validate_characters(sha):
                    ret.add(sha)
    return ret

# the purpose of this is to allow execution of exportpatch and series_insert from somewhere else than the root of KSOURCE_GIT
def get_relative_ksource():
    d = os.path.dirname
    r = os.path.realpath
    return r(d(d(d(r(__file__)))))

def exportpatch(bs):
    patches = []
    ksource_dir = get_relative_ksource()
    banlist, hashdict = set(), dict()
    try:
        banlist = get_banlist(ksource_dir + os.sep + 'blacklist.conf')
        hashdict = get_hashdict(ksource_dir + os.sep + 'patches.suse')
    except Exception as e:
        print(color_format(T_RED, e), file=sys.stderr)
        sys.exit(1)
    for b in bs:
        for s in b.shas:
            if s in banlist:
                print(color_format(T_PURPLE, f'{s} is in blacklist.conf'), file=sys.stderr)
            if s in hashdict:
                print(color_format(T_PURPLE, f'{s} is already in {hashdict[s]}'), file=sys.stderr)
    patch_dir = ksource_dir + os.sep + 'patches.suse'
    with ProcessPoolExecutor() as executor:
        job_descs = [(get_exportpatch_string([f'bsc#{b.data.id}', b.cve], s, patch_dir) + '\n', None) for b in bs for s in b.shas if s not in banlist and s not in hashdict]
        futures = { executor.submit(one_job, job): job for job in job_descs }
        for f in as_completed(futures):
            try:
                res = f.result()
                yield patch_dir + os.sep + res.stdout
                patches.append('patches.suse' + os.sep + res.stdout.rstrip())
            except subprocess.CalledProcessError as e:
                print(color_format(T_RED, f"{s} failed\n{e}\n{e.stderr}"))
            except Exception as e:
                print(color_format(T_RED, f"{s} failed\n{e}"), file=sys.stderr)
    try:
        res = one_job((get_insert_string(ksource_dir, " ".join(patches)) + '\n', None), ksource_dir)
        if res.stderr:
            print(color_format(T_RED, res.stderr.rstrip()), file=sys.stderr)
    except subprocess.CalledProcessError as e:
        print(color_format(T_RED, f"{bs} failed\n{e}\n{e.stderr}"))
    except Exception as e:
        print(color_format(T_RED, f"{bs} failed\n{e}"), file=sys.stderr)

# There are two kind of parallelism in this script.  One is below (the class ParallelRunner) and it takes care of parallel check-kernel-fix
# invocation and the other is above (the function exportpatch) that takes care of the parallel invocation of the script with the same name.
# Both are using the same technique: ProcessPoolExecutor that execute a bunch of futures.
# However the logic of the former is much more involved, so I have never unified them.
# On top of that, check-kernel-fix is very heavyweight and limited by memory rather than available cores.
# It has been observed that running more than 5 workers can trigger OOM.  So, user has an option to limit parallelism with -j option.
# On the other hand the exportpatch is very lightweight and its output very simple, so it can be executed more carelessly.
# Plus there's an additional step of running series_insert on the result of all the exportpatch invocations.

def output_exists(f, outdir, nopdir):
    if os.path.isfile(f):
        return f
    elif nopdir and os.path.isfile(nopdir + f[len(outdir):]):
        return nopdir + f[len(outdir):]
    return ''

class ParallelRunner:
    def __init__(self, blist, directory_pair, j):
        self.failures = [ f"# {make_url(b.data.id)} # NO CVE NUMBER" for b in blist if not b.cve ]
        self.jobs = [ ( f"./scripts/check-kernel-fix {'-s ' + b.score if b.score else '      '} -b {b.data.id} {b.cve}"
                        if b.data.status_whiteboard else f"./scripts/check-kernel-fix -b {b.data.id} {b.cve}",
                        directory_pair[0] + os.sep + f"kss.{b.cve}.bsc{b.data.id}")
                      for b in blist if b.cve ]
        self.parallel = bool(j)
        self.n_workers = j if j and j != -1 else multiprocessing.cpu_count()
        self.out_idr = directory_pair[0]
        self.nop_dir = None if len(directory_pair) < 2 else directory_pair[1]

    def __call__(self):
        for fail in self.failures:
            print(fail, file=sys.stderr)
        if not self.parallel:
            for script, path in self.jobs:
                yield f'{script} > {path}\n'
            return
        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            mod_jobs = []
            for job in self.jobs:
                output_f = output_exists(job[1], self.out_idr, self.nop_dir)
                if bool(output_f):
                    yield color_format(T_BLUE, f"Skipping '{job[0]} > {output_f}' ... (already exists)\n")
                else:
                    mod_jobs.append(job)
                    yield color_format(T_PURPLE, f"Schedule '{job[0]} > {job[1]}' ... (with {self.n_workers} workers...)\n")
            futures = { executor.submit(one_job, job): job for job in mod_jobs }
            for f in as_completed(futures):
                job = futures[f]
                file_to_store = str(job[1])
                yield color_format(T_YELLOW, f"$ {job[0]} > {job[1]}\n")
                try:
                    res = f.result()
                    yield res.stdout
                    if res.stderr:
                        yield color_format(T_RED, res.stderr)
                    yield color_format(T_PURPLE, f"Store '{job[1]}' ...\n")
                    if self.nop_dir and ('NO ACTION NEEDED: ' in res.stdout or 'NO CODESTREAM AFFECTED' in res.stdout):
                        file_to_store = self.nop_dir + file_to_store[len(self.out_idr):]
                        yield color_format(T_PURPLE, f"Moving from {self.out_idr} to {self.nop_dir}\n")
                    store_into_file(file_to_store, res.stdout)
                except subprocess.CalledProcessError as e:
                    store_into_file(file_to_store, e.stderr)
                    yield color_format(T_RED, f"{job} failed\n{e}\n{e.stderr}")
                except Exception as e:
                    print(color_format(T_RED, f"{job} failed\n{e}"), file=sys.stderr)

def one_job(job, cwd=None):
    return subprocess.run(job[0], cwd=cwd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True, universal_newlines=True)

def store_into_file(path, content):
    try:
        with open(path, 'w') as f:
            f.write(content)
    except Exception as e:
        print(color_format(T_RED, f"failed to store a file {path}: {e}"), file=sys.stderr)

@ExitOnExceptionHandle(" must be at least 1")
def handle_parallelism(njobs, ckf):
    if not njobs:
        return None
    if not ckf:
        print("--jobs (-j) doesn't make sense without --check-kernel-fix (-0)", file=sys.stderr)
        sys.exit(1)
    r = int(njobs)
    if r == -1:
        return r
    if r < 1:
        raise Exception("r < 1")
    return r

@ExitOnExceptionHandle(": ")
def handle_grep(regex):
    if not regex:
        return None
    return re.compile(regex, flags=re.IGNORECASE)

@ExitOnExceptionHandle(": expected format is Q:R were Q > 0 and R < Q")
def handle_modulo(qr):
    if not qr:
        return 0, 0
    q, r = qr.split(':')
    q = int(q)
    r = int(r)
    if q < 1 or r < 0 or r >= q:
        raise Exception("q < 1 or r < 0 or r >= q")
    return (q, r)

@ExitOnExceptionHandle(": expected format is X or X:Y were X >= 0 and Y >= 0 and X >= Y")
def handle_ages(ages):
    if not ages:
        return (None, None)
    if ':' in ages:
        x, y = ages.split(':')
        x = int(x)
        y = int(y)
        if x < y or x < 0 or y < 0:
            raise Exception("x < y or x < 0 or y < 0")
        return (x, y)
    else:
        x = int(ages)
        if x < 0:
            raise Exception("x < 0")
        return (x, None)

def handle_bugs(bug_string):
    if not bug_string:
        return None
    ret = []
    for b in bug_string.split(','):
        try:
            if b.startswith('bsc#'):
                b = b[4:]
            bz_number = int(b)
            if bz_number < 1:
                raise Exception()
            ret.append(bz_number)
        except:
            print(f'{b} is not a valid bz number', file=sys.stderr)
    if not ret:
        sys.exit(1)
    return ret

def handle_cves(cve_string):
    if not cve_string:
        return None
    ret = []
    for c in cve_string.split(','):
        m = re.match(CVE_PATTERN, c)
        if not m:
            print(f'{c} is not a valid CVE number', file=sys.stderr)
            continue
        ret.append(c)
    if not ret:
        sys.exit(1)
    return ret

@ExitOnExceptionHandle(" is not a valid CVE score")
def handle_score(score_string):
    if not score_string:
        return None
    ret = [ int(n) for n in score_string.split('.') ]
    if len(ret) == 1 and ret[0] >= 0 and ret[0] <= 10:
        ret.append(0)
        return ret
    elif len(ret) == 2 and ret[0] >= 0 and ret[0] <= 10 and ret[1] >= 0 and ret[1] <= 9:
        return ret
    raise Exception()

# stateful options (whose order matter); the other options are handled by handle_* functions
class SortActions(argparse.Action):
    key_functions = []
    settings = { 'changes': False, 'score': False, 'subsystem': False }
    def __call__(self, parser, namespace, values, option_string = None):
        if option_string in ["-S", "--sort-score"]:
            self.key_functions.append(lambda x: [ int(n) for n in x.score.split('.') ])
            self.settings['score'] = True
        elif option_string in ["-N", "--sort-cve"]:
            self.key_functions.append(lambda x: [ int(n) for n in re.findall(r'\d+', x.cve) ])
        elif option_string in ["-t", "--sort-time"]:
            self.key_functions.append(lambda x: x.mtime)
        elif option_string in ["-M", "--sort-changes"]:
            self.key_functions.append(lambda x: x.changes)
            self.settings['changes'] = True
        elif option_string == '--sort-age':
            self.key_functions.append(lambda x: x.age)
        elif option_string == '--sort-subsystem':
            self.settings['subsystem'] = True
            self.key_functions.append(lambda x: x.sgm_data.get('subsystem', ''))
        elif option_string == '--sort-deadline':
            self.key_functions.append(lambda x: - x.overdue if x.overdue else sys.maxsize)

def parse_args():
    parser = argparse.ArgumentParser(description="KSS Dashboard")
    c_group = parser.add_mutually_exclusive_group()
    c_group.add_argument("-c", "--colors", help="show colors unconditionally (by default they show only in terminal)", action="store_true", default=False)
    c_group.add_argument("-C", "--no-colors", help="do not show colors unconditionally (by default they show only in terminal)", action="store_true", default=False)
    w_group = parser.add_mutually_exclusive_group()
    w_group.add_argument("-w", "--whiteboard", help="show only bugs with whiteboard set (thus CVSS score set)", action="store_true", default=False)
    w_group.add_argument("-W", "--no-whiteboard", help="show only bugs with whiteboard not set (thus no CVSS score)", action="store_true", default=False)
    s_group = parser.add_mutually_exclusive_group()
    s_group.add_argument("-s", "--short", help="show only bug headers (one bug per line; no fixes tags)", action="store_true", default=False)
    s_group.add_argument("-p", "--patch", help="show the entire fixing patch; incompatible with --short", action="store_true", default=False)
    s_group.add_argument("--breakers", help="show all the breaking changes; incompatible with --short or --patch", action="store_true", default=False)
    s_group.add_argument("-f", "--files", help="show paths from the patch; incompatible with --short and --grep-paths", action="store_true", default=False)
    s_group.add_argument("-u", "--url", help="show bug url (SMASH)", action="store_true", default=False)
    s_group.add_argument("-A", "--analyze-vector", help="show all CVSSv3.1 components (https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator) and nothing else", action="store_true", default=False)
    s_group.add_argument("-H", "--history", help="show bug histories", action="store_true", default=False)
    s_group.add_argument("--comments-short", help="show comment links with authors and dates", action="store_true", default=False)
    s_group.add_argument("-L", "--comments-long", help="show all human comments", action="store_true", default=False)
    s_group.add_argument("--comments-smash", help="show comments made by SMASH", action="store_true", default=False)
    s_group.add_argument("-0", "--check-kernel-fix", help="prints ./scripts/check-kernel-fix cli; takes an optional argument <directory>[,nop_directory]", nargs='?', type=str, default='', const='/tmp')
    s_group.add_argument("-1", "--exportpatch", help="run exportpatch in parallel and series_insert command for all the backports with correct references", action="store_true", default=False)
    s_group.add_argument("-2", "--suse-get-maintainers", help="prints subsystem and resposible maintainers", action="store_true", default=False)
    s_group.add_argument("--deadlines", help="show approaching deadlines for the bugs ", action="store_true", default=False)
    parser.add_argument("--minimal-score", help="show only bugs with the score equal or above", default=None, type=str)
    parser.add_argument("-j", "--jobs", help="runs ./scripts/check-kernel-fix in parallel; takes an optional argument <number of workers>; to be used with -0", nargs='?', type=str, default=None, const='-1')
    parser.add_argument("-i", "--stats-info", help="show patch statistics; # of changed files, # of added lines, # of removed lines", action="store_true", default=False)
    parser.add_argument("-b", "--birth-time", help="show the reported time instead of the last modification time", action="store_true", default=False)
    parser.add_argument("-S", "--sort-score", help="sort bugs by CVSS score instead of their bug ids, implies -w (--whiteboard)", action=SortActions, nargs=0)
    parser.add_argument("-N", "--sort-cve", help="sort bugs by CVE number instead of their bug ids", action=SortActions, nargs=0)
    parser.add_argument("-t", "--sort-time", help="sort bugs by the last comment (excluding the bots)", action=SortActions, nargs=0)
    parser.add_argument("-M", "--sort-changes", help="sort by the number of insertion/deletion introduced by the patch", action=SortActions, nargs=0)
    parser.add_argument("--affected", help="print affected codestreams according to SMASH if available", action="store_true", default=False)
    parser.add_argument("--sort-age", help="sort bugs by the age in days; useful with --ages and --birth-time", action=SortActions, nargs=0)
    parser.add_argument("--sort-subsystem", help="sort bugs by the subsystem (according to suse-get-maintainers)", action=SortActions, nargs=0)
    parser.add_argument("--sort-deadline", help="sort bugs by the deadline determined by the CVE score", action=SortActions, nargs=0)
    parser.add_argument("-r", "--reverse", help="sort bugs in reverse order (descending)", action="store_true", default=False)
    q_group = parser.add_mutually_exclusive_group()
    q_group.add_argument("-e", "--email", help="instead of incoming CVE queue, display CVE queue for another email; including info about the last comment for each bug;"\
                         " can be also taken from the envvar $BUGZILLA_ACCOUNT_EMAIL", default='kernel-bugs@suse.de', nargs='?',
                         const=os.environ.get('BUGZILLA_ACCOUNT_EMAIL', '__empty-env-var__'), type=str)
    q_group.add_argument("-k", "--cve-kpm", help="show CVE Kernel Patch Monkey queue", action="store_true", default=False)
    q_group.add_argument("-q", "--assigned-queue", help="show already assigned bugs", action="store_true", default=False)
    q_group.add_argument("-d", "--done", help="show bugs already switched to the security team; BEWARE: this might take a long time to execute as the queue is quite long",
                         action="store_true", default=False)
    q_group.add_argument("-B", "--bugs", help="show only specific bugs; takes a comma separated list of bug numbers, they must be security incidents", default=None, type=str)
    q_group.add_argument("-V", "--cves", help="show only specific CVE bugs; takes a comma separated list of CVE numbers", default=None, type=str)
    parser.add_argument("-z", "--zdi", help="show only ZDI advisories", action="store_true", default=False)
    parser.add_argument("-o", "--overdue", help="show only bugs that are overdue; 30 days for score >= 7.0 and 90 days fro score >= 4.0", action="store_true", default=False)
    parser.add_argument("-R", "--rejected", help="show rejected CVEs if any", action="store_true", default=False)
    parser.add_argument("-g", "--grep", help="grep summary for REGEX and show only bugs that match it", default=None, type=str)
    parser.add_argument("-G", "--grep-paths", help="grep fix commit paths for REGEX and show only bugs with fix commits that match it", default=None, type=str)
    parser.add_argument("-P", "--grep-patch", help="grep fix commit patch for REGEX and show only bugs with fix commits that match it", default=None, type=str)
    parser.add_argument("-T", "--grep-comments", help="grep bugzilla comments and show only bugs that match including the matching comments (highlighted)", default=None, type=str)
    parser.add_argument("--grep-subsystem", help="grep subsystem the fix belongs to (according to suse-get-maintainers) and show only bugs that match", default=None, type=str)
    parser.add_argument("--grep-maintainers", help="grep maintainers the fix belongs to (according to suse-get-maintainers) and show only bugs that match", default=None, type=str)
    parser.add_argument("-a", "--ages", help="show only tickets of a particular age in days; X means X days old or older and X:Y means betwen X and Y days old, inclusive", default=None, type=str)
    parser.add_argument("-m", "--modulo", help="takes two integers in the format \"Q:R\" and shows only bugs with IDs that satisfy N %% Q == R", default=None, type=str)
    parser.add_argument("-Z", "--pager", help="user pager for output (less -iR)", action="store_true", default=False)
    parser.add_argument("--cve-branch", help="which branch we care about in $VULNS_GIT repository (by default origin/master)", default='origin/master', type=str)
    parser.add_argument("--rest", help="Use REST API instead of XMLRPC APII (experimental, for debugging purposes)", action="store_true", default=False)
    parser.add_argument("--debug", help="Enable Bugzilla RPC debugging", action="store_true", default=False)
    return parser.parse_args()

# the main output producing generator that yields from other generators, recursively
def generate_output(linux_git, bugs, args, key_functions, jobs):
    if args.check_kernel_fix:
        ckf_dirs_pair = args.check_kernel_fix.split(',')[:2]
        sucess = False
        for d in ckf_dirs_pair:
            if os.path.isdir(d) and os.access(d, os.W_OK):
                sucess = True
                continue
            try:
                os.mkdir(d)
                sucess = True
            except FileExistsError:
                pass
            except PermissionError as e:
                print(f'{color_format(T_RED, "failed to create directory")}: {e}', file=sys.stderr)
                sys.exit(1)
        if not sucess:
            print(f'{color_format(T_RED, "failed to create any directory from")}: {args.check_kernel_fix}', file=sys.stderr)
            sys.exit(1)
        pr = ParallelRunner(sorted(bugs, key=lambda x: [ f(x) for f in key_functions], reverse=args.reverse), ckf_dirs_pair, jobs)
        yield from pr()
        return
    patches_to_export = []
    for n, b in enumerate(sorted(bugs, key=lambda x: [ f(x) for f in key_functions], reverse=args.reverse), 1):
        if args.exportpatch:
            patches_to_export.append(b)
        else:
            yield from show_bug(n, b, linux_git, args)
    if patches_to_export:
        yield from exportpatch(patches_to_export)

# this relies on suse-get-maintainers producing correct JSON, if this ever breaks suse-get-maintainers must be fixed
def load_sgm_data(cves):
    res = None
    try:
        sgm_args = ['suse-get-maintainers', '--names', '--only_maintainers', '--json' ]
        for c in cves:
            sgm_args.extend(['-c', c])
        res = subprocess.run(args=sgm_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True, universal_newlines=True)
    except subprocess.CalledProcessError as e:
        print(color_format(T_RED, f'{e}\n{" ".join(sgm_args)}'), file=sys.stderr)
        if hasattr(e, 'stderr'):
            print(e.stderr, file=sys.stderr, end='')
        sys.exit(1)
    except Exception as e:
        print(color_format(T_RED, f'Please install suse-get-maintainers from kernel-tools repo! ({e})'), file=sys.stderr)
        sys.exit(1)
    data = json.loads(res.stdout)
    return { c['cve']: c for c in data }

def main():
    args = parse_args()
    if args.debug:
        import logging
        logging.basicConfig(level=logging.DEBUG)
    global time_format
    # for whatever reason REST API is slower than XMLRPC API for multiple bug queries, that's why it's not the default and it's used for debugging purposes only
    # I'm not sure whether it's python-bugzilla problem or inherent REST API problem
    # + the time format is different, hence this ugly hack
    if args.rest:
        time_format = TIME_FORMAT_REST
    else:
        time_format = TIME_FORMAT_XML
    # all stateless (it doesn't matter in which order the options are provided on the command line) option sanitizing functions are named handle_*
    grep = handle_grep(args.grep)
    grep_paths = handle_grep(args.grep_paths)
    grep_patch = handle_grep(args.grep_patch)
    grep_comments = handle_grep(args.grep_comments)
    grep_subsystem = handle_grep(args.grep_subsystem)
    grep_maintainers = handle_grep(args.grep_maintainers)
    q, r = handle_modulo(args.modulo)
    x, y = handle_ages(args.ages)
    email = handle_email(args.email)
    bug_list = handle_bugs(args.bugs)
    cve_list = handle_cves(args.cves)
    min_score = handle_score(args.minimal_score)
    if args.cve_kpm:
        email = 'cve-kpm@suse.de'
    if args.done:
        email = 'security-team@suse.de'
    if args.assigned_queue:
        email = None
    jobs = handle_parallelism(args.jobs, args.check_kernel_fix)
    bzapi = BZApi(args.rest)
    # here the magic happens when it comes to query to the bugzilla
    bugs = bzapi.fetch_bugs(email, bug_list, cve_list)
    linux_git = MainlineRepo()

    # by default we show colors in terminal only, see the top of the script, but here we can force it one way or the other
    global show_colors
    if args.colors:
        show_colors = True
    elif args.no_colors:
        show_colors = False

    # class BugData is our wrapper around the python-bugzilla representation of a bug, since we want to keep additional state
    bugs = [ BugData(b) for b in bugs ]

    # fetch_cves function might need rework in case the kernel CNA get strange ideas about the format of their repo again
    cves = [ b.cve for b in bugs if b.cve ]
    published, rejected, breakers = fetch_cves(cves, args.cve_branch)
    validate_hashes(published)
    validate_hashes(rejected)
    validate_hashes(breakers)

    if args.affected:
        global smash_token
        smash_token = get_smash_token()

    for b in bugs:
        b.update_vulns(published, rejected, breakers)

    # in the following lines we try to be smart, in case some options are not provided, then some work doesn't need to happen
    if args.suse_get_maintainers or args.grep_subsystem or args.grep_maintainers or SortActions.settings['subsystem']:
        cves_for_sgm = [ b.cve for b in bugs ]
        # we invoke suse-get-maintainers only once for all cves and load its JSON output, which is easy to parse within python
        sgm_data = load_sgm_data(cves_for_sgm)
        for b in bugs:
            if b.cve:
                b.sgm_data = sgm_data.get(b.cve, {})

    if min_score:
        bugs = [ b for b in bugs if b.score and [ int(n) for n in b.score.split('.') ] >= min_score ]

    if args.rejected:
        bugs = [ b for b in bugs if b.is_rejected ]

    if args.zdi:
        bugs = [ b for b in bugs if b.zdi ]

    if args.overdue:
        bugs = [ b for b in bugs if b.overdue and b.overdue > 0 ]

    if args.whiteboard or SortActions.settings['score']:
        bugs = [ b for b in bugs if b.data.status_whiteboard ]
    elif args.no_whiteboard:
        bugs = [ b for b in bugs if not b.data.status_whiteboard ]

    if args.exportpatch:
        bugs = [ b for b in bugs if b.shas and b.data.status_whiteboard ]

    if grep:
        bugs = [ b for b in bugs if re.search(grep, b.data.summary) ]

    if grep_paths:
        bugs = [ b for b in bugs if b.grep_paths(grep_paths, linux_git) ]
        args.files = True

    if grep_patch:
        bugs = [ b for b in bugs if b.grep_patch(grep_patch, linux_git) ]

    if grep_subsystem:
        bugs = [ b for b in bugs if b.grep_subsystem(grep_subsystem, linux_git) ]

    if grep_maintainers:
        bugs = [ b for b in bugs if b.grep_maintainers(grep_maintainers, linux_git) ]

    if q:
        bugs = [ b for b in bugs if b.data.id % q == r ]

    # in here we try to be smart again, after narrowing bug selection as much as possible, it's time to fetch the comments
    # it's much faster to do it in one go than to try to fetch them bug by bug for non-trivial amount of bugs
    bzapi.fetch_comments(bugs)

    if grep_comments:
        bugs = [ b for b in bugs if b.grep_comments(grep_comments) ]

    if args.history:
        bzapi.fetch_history(bugs)

    if x is not None and y is None:
        bugs = [ b for b in bugs if b.age >= x ]
    elif x is not None and y is not None:
        bugs = [ b for b in bugs if b.age <= x and b.age >= y ]

    if args.analyze_vector and not args.short:
        bugs = [ b.with_score_vector() for b in bugs if b.data.status_whiteboard ]
        args.short = True

    if SortActions.settings['changes']:
        bugs = [ b for b in bugs if b.get_stats(linux_git) > 0 ]
    # --sort-* options handling: these are the only options whose order on the command line matter, hence we need to construct
    # global sort function by appending invididual sort functions in the reverse order they are provided on the command line
    # the final step is within generate_output when the order of sort functions is finally reversed before all the functions being invoked
    # i.e. "--sort-cve --sort-score" is different from "--sort-score --sort-cve"
    SortActions.key_functions.append(lambda x: x.data.id)

    # by convention error messages are sent to STDERR via print(..., file=sys.stderr)
    # normal output is yielded as a string from generators that are recusively collected via "yield from" into the main generator: generate_output
    # below we iterate over the main generator and send text lines to either STDOUT or to the PAGER program
    if args.pager:
        import subprocess
        pager = subprocess.Popen(['less', '-i', '-R'], stdin=subprocess.PIPE, universal_newlines=True)
        dst = pager.stdin
    else:
        dst = sys.stdout

    try:
        for l in generate_output(linux_git, bugs, args, SortActions.key_functions, jobs):
            dst.write(l)
    except BrokenPipeError:
        pass

    if args.pager:
        dst.close()
        pager.wait()

main()
