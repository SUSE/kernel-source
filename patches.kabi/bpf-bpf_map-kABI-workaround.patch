From: Shung-Hsi Yu <shung-hsi.yu@suse.com>
Subject: kABI: bpf: struct bpf_map kABI workaround
Patch-mainline: never, kabi
References: bsc#1220251 bsc#1232435 CVE-2023-52447 CVE-2024-50063

- Commit 876673364161 ("bpf: Defer the free of inner map when necessary") adds
  a new field into struct bpf_map. Luckily it falls within the padding, so
  nothing else is needed other than hiding it from __GENKSYMS__.

- Upstream commit 28ead3eaabc1 ("bpf: Prevent tail call between progs attached
  to different hooks") changed struct bpf_map to add attach_func_proto pointer
  and thus breaks kABI.

  Luckily we have kABI padding inplace right within struct bpf_map, hence all
  that's needed is to move the newly added field to where the padding is
  located and kABI will be preserved.

Signed-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>
---
 include/linux/bpf.h  |   73 ++++++++++++++++++++++++++++++++++++++++++++++++++-
 kernel/bpf/core.c    |    4 +-
 kernel/bpf/syscall.c |    1 
 3 files changed, 75 insertions(+), 3 deletions(-)

--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -230,11 +230,18 @@ struct bpf_map {
 	 */
 	atomic64_t refcnt ____cacheline_aligned;
 	atomic64_t usercnt;
+#ifndef __GENKSYMS__
 	/* rcu is used before freeing and work is only used during freeing */
 	union {
+#endif
 		struct work_struct work;
+#ifndef __GENKSYMS__
 		struct rcu_head rcu;
 	};
+      /* Assert union of rcu_head and work_struct won't be larger than size
+       * of the original work_struct, thus breaking kABI */
+      static_assert(sizeof(struct work_struct) >= sizeof(struct rcu_head));
+#endif
 	struct mutex freeze_mutex;
 	atomic64_t writecnt;
 	/* 'Ownership' of program-containing map is claimed by the first program
@@ -243,7 +250,6 @@ struct bpf_map {
 	 * same prog type, JITed flag and xdp_has_frags flag.
 	 */
 	struct {
-		const struct btf_type *attach_func_proto;
 		spinlock_t lock;
 		enum bpf_prog_type type;
 		bool jited;
@@ -251,10 +257,75 @@ struct bpf_map {
 	} owner;
 	bool bypass_spec_v1;
 	bool frozen; /* write-once; write-protected by freeze_mutex */
+#ifndef __GENKSYMS__
+	/* Fits into the hole in struct */
 	bool free_after_mult_rcu_gp;
+#endif
+#ifndef __GENKSYMS__
+	/* Uses the SUSE kABI padding */
+	const struct btf_type *owner_attach_func_proto;
+#else
 	void *suse_kabi_padding;
+#endif
 };
 
+static_assert(sizeof(struct work_struct) >= sizeof(struct rcu_head));
+
+struct __orig_bpf_map {
+	/* The first two cachelines with read-mostly members of which some
+	 * are also accessed in fast-path (e.g. ops, max_entries).
+	 */
+	const struct bpf_map_ops *ops ____cacheline_aligned;
+	struct bpf_map *inner_map_meta;
+#ifdef CONFIG_SECURITY
+	void *security;
+#endif
+	enum bpf_map_type map_type;
+	u32 key_size;
+	u32 value_size;
+	u32 max_entries;
+	u64 map_extra; /* any per-map-type extra fields */
+	u32 map_flags;
+	int spin_lock_off; /* >=0 valid offset, <0 error */
+	struct bpf_map_value_off *kptr_off_tab;
+	int timer_off; /* >=0 valid offset, <0 error */
+	u32 id;
+	int numa_node;
+	u32 btf_key_type_id;
+	u32 btf_value_type_id;
+	u32 btf_vmlinux_value_type_id;
+	struct btf *btf;
+#ifdef CONFIG_MEMCG_KMEM
+	struct mem_cgroup *memcg;
+#endif
+	char name[BPF_OBJ_NAME_LEN];
+	struct bpf_map_off_arr *off_arr;
+	/* The 3rd and 4th cacheline with misc members to avoid false sharing
+	 * particularly with refcounting.
+	 */
+	atomic64_t refcnt ____cacheline_aligned;
+	atomic64_t usercnt;
+	struct work_struct work;
+	struct mutex freeze_mutex;
+	atomic64_t writecnt;
+	/* 'Ownership' of program-containing map is claimed by the first program
+	 * that is going to use this map or by the first program which FD is
+	 * stored in the map to make sure that all callers and callees have the
+	 * same prog type, JITed flag and xdp_has_frags flag.
+	 */
+	struct {
+		spinlock_t lock;
+		enum bpf_prog_type type;
+		bool jited;
+		bool xdp_has_frags;
+	} owner;
+	bool bypass_spec_v1;
+	bool frozen; /* write-once; write-protected by freeze_mutex */
+	void *suse_kabi_padding;
+};
+/* Make sure our kABI workaround really does work */
+static_assert(sizeof(struct __orig_bpf_map) == sizeof(struct bpf_map));
+
 static inline bool map_value_has_spin_lock(const struct bpf_map *map)
 {
 	return map->spin_lock_off >= 0;
--- a/kernel/bpf/core.c
+++ b/kernel/bpf/core.c
@@ -2136,14 +2136,14 @@ bool bpf_prog_map_compatible(struct bpf_
 		map->owner.type  = prog_type;
 		map->owner.jited = fp->jited;
 		map->owner.xdp_has_frags = aux->xdp_has_frags;
-		map->owner.attach_func_proto = aux->attach_func_proto;
+		map->owner_attach_func_proto = aux->attach_func_proto;
 		ret = true;
 	} else {
 		ret = map->owner.type  == prog_type &&
 		      map->owner.jited == fp->jited &&
 		      map->owner.xdp_has_frags == aux->xdp_has_frags;
 		if (ret &&
-		    map->owner.attach_func_proto != aux->attach_func_proto) {
+		    map->owner_attach_func_proto != aux->attach_func_proto) {
 			switch (prog_type) {
 			case BPF_PROG_TYPE_TRACING:
 			case BPF_PROG_TYPE_LSM:
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -616,6 +616,7 @@ static void bpf_map_free_in_work(struct
 
 static void bpf_map_free_rcu_gp(struct rcu_head *rcu)
 {
+
 	bpf_map_free_in_work(container_of(rcu, struct bpf_map, rcu));
 }
 
