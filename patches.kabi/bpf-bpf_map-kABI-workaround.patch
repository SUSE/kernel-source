From: Shung-Hsi Yu <shung-hsi.yu@suse.com>
Subject: kABI: bpf: struct bpf_map kABI workaround
Patch-mainline: never, kabi
References: bsc#1220251 bsc#1232435 CVE-2023-52447 CVE-2024-50063

- Commit 876673364161 ("bpf: Defer the free of inner map when necessary") adds
  a new field into struct bpf_map. Luckily it falls within the padding, so
  nothing else is needed other than hiding it from __GENKSYMS__.

- Upstream commit 28ead3eaabc1 ("bpf: Prevent tail call between progs attached
  to different hooks") changed struct bpf_map to add attach_func_proto pointer
  and thus breaks kABI.

  Luckily we have kABI padding inplace right within struct bpf_map, hence all
  that's needed is to move the newly added field to where the padding is
  located and kABI will be preserved.

Signed-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>
---
 include/linux/bpf.h  |   14 +++++++++++++-
 kernel/bpf/core.c    |    4 ++--
 kernel/bpf/syscall.c |    1 +
 3 files changed, 16 insertions(+), 3 deletions(-)

--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -230,11 +230,18 @@ struct bpf_map {
 	 */
 	atomic64_t refcnt ____cacheline_aligned;
 	atomic64_t usercnt;
+#ifndef __GENKSYMS__
 	/* rcu is used before freeing and work is only used during freeing */
 	union {
+#endif
 		struct work_struct work;
+#ifndef __GENKSYMS__
 		struct rcu_head rcu;
 	};
+      /* Assert union of rcu_head and work_struct won't be larger than size
+       * of the original work_struct, thus breaking kABI */
+      static_assert(sizeof(struct work_struct) >= sizeof(struct rcu_head));
+#endif
 	struct mutex freeze_mutex;
 	atomic64_t writecnt;
 	/* 'Ownership' of program-containing map is claimed by the first program
@@ -243,7 +250,6 @@ struct bpf_map {
 	 * same prog type, JITed flag and xdp_has_frags flag.
 	 */
 	struct {
-		const struct btf_type *attach_func_proto;
 		spinlock_t lock;
 		enum bpf_prog_type type;
 		bool jited;
@@ -251,10 +257,16 @@ struct bpf_map {
 	} owner;
 	bool bypass_spec_v1;
 	bool frozen; /* write-once; write-protected by freeze_mutex */
+#ifndef __GENKSYMS__
 	bool free_after_mult_rcu_gp;
+	const struct btf_type *owner_attach_func_proto;
+#else
 	void *suse_kabi_padding;
+#endif
 };
 
+static_assert(sizeof(struct work_struct) >= sizeof(struct rcu_head));
+
 static inline bool map_value_has_spin_lock(const struct bpf_map *map)
 {
 	return map->spin_lock_off >= 0;
--- a/kernel/bpf/core.c
+++ b/kernel/bpf/core.c
@@ -2136,14 +2136,14 @@ bool bpf_prog_map_compatible(struct bpf_
 		map->owner.type  = prog_type;
 		map->owner.jited = fp->jited;
 		map->owner.xdp_has_frags = aux->xdp_has_frags;
-		map->owner.attach_func_proto = aux->attach_func_proto;
+		map->owner_attach_func_proto = aux->attach_func_proto;
 		ret = true;
 	} else {
 		ret = map->owner.type  == prog_type &&
 		      map->owner.jited == fp->jited &&
 		      map->owner.xdp_has_frags == aux->xdp_has_frags;
 		if (ret &&
-		    map->owner.attach_func_proto != aux->attach_func_proto) {
+		    map->owner_attach_func_proto != aux->attach_func_proto) {
 			switch (prog_type) {
 			case BPF_PROG_TYPE_TRACING:
 			case BPF_PROG_TYPE_LSM:
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -616,6 +616,7 @@ static void bpf_map_free_in_work(struct
 
 static void bpf_map_free_rcu_gp(struct rcu_head *rcu)
 {
+
 	bpf_map_free_in_work(container_of(rcu, struct bpf_map, rcu));
 }
 
