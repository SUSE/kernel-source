From: Daniel Wagner <dwagner@suse.de>
Subject: kabi: fix group_cpus_evenly
Patch-mainline: Never, kABI workaround
References: bsc#1236897

Bring back the old verion of group_cpus_evenly.

Signed-off-by: Daniel Wagner <dwagner@suse.de>

---
 block/blk-mq-cpumap.c        |    4 ++--
 drivers/virtio/virtio_vdpa.c |    2 +-
 include/linux/group_cpus.h   |    5 +++--
 kernel/irq/affinity.c        |    2 +-
 lib/group_cpus.c             |   16 ++++++++++++----
 5 files changed, 19 insertions(+), 10 deletions(-)

--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -81,7 +81,7 @@ static bool blk_mq_map_hk_queues(struct
 		return false;
 
 	/* map housekeeping cpus to matching hardware context */
-	hk_masks = group_cpus_evenly(qmap->nr_queues, &nr_masks);
+	hk_masks = group_cpus_evenly_nm(qmap->nr_queues, &nr_masks);
 	if (!hk_masks)
 		goto fallback;
 
@@ -128,7 +128,7 @@ void blk_mq_map_queues(struct blk_mq_que
 	if (blk_mq_map_hk_queues(qmap))
 		return;
 
-	masks = group_cpus_evenly(qmap->nr_queues, &nr_masks);
+	masks = group_cpus_evenly_nm(qmap->nr_queues, &nr_masks);
 	if (!masks) {
 		for_each_possible_cpu(cpu)
 			qmap->mq_map[cpu] = qmap->queue_offset;
--- a/drivers/virtio/virtio_vdpa.c
+++ b/drivers/virtio/virtio_vdpa.c
@@ -328,7 +328,7 @@ create_affinity_masks(unsigned int nvecs
 		unsigned int this_vecs = affd->set_size[i];
 		unsigned int nr_masks;
 		int j;
-		struct cpumask *result = group_cpus_evenly(this_vecs, &nr_masks);
+		struct cpumask *result = group_cpus_evenly_nm(this_vecs, &nr_masks);
 
 		if (!result) {
 			kfree(masks);
--- a/include/linux/group_cpus.h
+++ b/include/linux/group_cpus.h
@@ -9,7 +9,8 @@
 #include <linux/kernel.h>
 #include <linux/cpu.h>
 
-struct cpumask *group_cpus_evenly(unsigned int numgrps,
-				  unsigned int *nummasks);
+struct cpumask *group_cpus_evenly(unsigned int numgrps);
+struct cpumask *group_cpus_evenly_nm(unsigned int numgrps,
+				     unsigned int *nummasks);
 
 #endif
--- a/kernel/irq/affinity.c
+++ b/kernel/irq/affinity.c
@@ -72,7 +72,7 @@ irq_create_affinity_masks(unsigned int n
 		unsigned int this_vecs = affd->set_size[i];
 		unsigned int nr_masks;
 		int j;
-		struct cpumask *result = group_cpus_evenly(this_vecs, &nr_masks);
+		struct cpumask *result = group_cpus_evenly_nm(this_vecs, &nr_masks);
 
 		if (!result) {
 			kfree(masks);
--- a/lib/group_cpus.c
+++ b/lib/group_cpus.c
@@ -491,8 +491,8 @@ static struct cpumask *group_mask_cpus_e
  * argument is used with managed_irq option. In this case only the
  * housekeeping CPUs are considered.
  */
-struct cpumask *group_cpus_evenly(unsigned int numgrps,
-				  unsigned int *nummasks)
+struct cpumask *group_cpus_evenly_nm(unsigned int numgrps,
+				     unsigned int *nummasks)
 {
 	if (housekeeping_enabled(HK_TYPE_MANAGED_IRQ)) {
 		return group_mask_cpus_evenly(numgrps,
@@ -503,8 +503,8 @@ struct cpumask *group_cpus_evenly(unsign
 	return group_possible_cpus_evenly(numgrps, nummasks);
 }
 #else /* CONFIG_SMP */
-struct cpumask *group_cpus_evenly(unsigned int numgrps,
-				  unsigned int *nummasks)
+struct cpumask *group_cpus_evenly_nm(unsigned int numgrps,
+				     unsigned int *nummasks)
 {
 	struct cpumask *masks;
 
@@ -521,4 +521,12 @@ struct cpumask *group_cpus_evenly(unsign
 	return masks;
 }
 #endif /* CONFIG_SMP */
+EXPORT_SYMBOL_GPL(group_cpus_evenly_nm);
+
+struct cpumask *group_cpus_evenly(unsigned int numgrps)
+{
+	unsigned int nummasks;
+
+	return group_cpus_evenly_nm(numgrps, &nummasks);
+}
 EXPORT_SYMBOL_GPL(group_cpus_evenly);
