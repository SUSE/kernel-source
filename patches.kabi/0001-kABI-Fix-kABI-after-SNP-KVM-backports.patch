From: Joerg Roedel <jroedel@suse.de>
Date: Mon, 24 Jun 2024 13:39:15 +0200
Subject: kABI: Fix kABI after SNP KVM backports
Patch-mainline: Never, kABI fix
References: jsc#PED-5122

Signed-off-by: Joerg Roedel <jroedel@suse.de>
---
 arch/x86/include/asm/kvm_host.h | 11 ++++++-----
 arch/x86/kvm/mmu/mmu.c          |  2 +-
 arch/x86/kvm/svm/svm.c          |  4 ++--
 arch/x86/kvm/x86.c              | 30 +++++++++++++++---------------
 include/linux/cc_platform.h     |  2 ++
 include/linux/kvm_host.h        |  5 +++++
 include/uapi/linux/kvm.h        |  2 ++
 7 files changed, 33 insertions(+), 23 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f7d37dfc2ba7..0971d381a240 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1288,14 +1288,12 @@ enum kvm_apicv_inhibit {
 };
 
 struct kvm_arch {
+	unsigned long vm_type;
 	unsigned long n_used_mmu_pages;
 	unsigned long n_requested_mmu_pages;
 	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
 	u8 mmu_valid_gen;
-	u8 vm_type;
-	bool has_private_mem;
-	bool has_protected_state;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
@@ -1787,7 +1785,6 @@ struct kvm_x86_ops {
 	void (*enable_smi_window)(struct kvm_vcpu *vcpu);
 #endif
 
-	int (*dev_get_attr)(u32 group, u64 attr, u64 *val);
 	int (*mem_enc_ioctl)(struct kvm *kvm, void __user *argp);
 	int (*mem_enc_register_region)(struct kvm *kvm, struct kvm_enc_region *argp);
 	int (*mem_enc_unregister_region)(struct kvm *kvm, struct kvm_enc_region *argp);
@@ -1817,9 +1814,13 @@ struct kvm_x86_ops {
 	gva_t (*get_untagged_addr)(struct kvm_vcpu *vcpu, gva_t gva, unsigned int flags);
 
 	void *(*alloc_apic_backing_page)(struct kvm_vcpu *vcpu);
+
+#ifndef __GENKSYMS__
+	int (*dev_get_attr)(u32 group, u64 attr, u64 *val);
 	int (*gmem_prepare)(struct kvm *kvm, kvm_pfn_t pfn, gfn_t gfn, int max_order);
 	void (*gmem_invalidate)(kvm_pfn_t start, kvm_pfn_t end);
 	int (*private_max_mapping_level)(struct kvm *kvm, kvm_pfn_t pfn);
+#endif
 };
 
 struct kvm_x86_nested_ops {
@@ -2170,12 +2171,12 @@ void kvm_configure_mmu(bool enable_tdp, int tdp_forced_root_level,
 
 
 #ifdef CONFIG_KVM_PRIVATE_MEM
-#define kvm_arch_has_private_mem(kvm) ((kvm)->arch.has_private_mem)
+#define kvm_arch_has_private_mem(kvm) ((kvm)->has_private_mem)
 #else
 #define kvm_arch_has_private_mem(kvm) false
 #endif
 
-#define kvm_arch_has_readonly_mem(kvm) (!(kvm)->arch.has_protected_state)
+#define kvm_arch_has_readonly_mem(kvm) (!(kvm)->has_protected_state)
 
 static inline u16 kvm_read_ldt(void)
 {
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 2a361bbfcc88..1fd9a9b3271e 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -3375,7 +3375,7 @@ static bool page_fault_can_be_fast(struct kvm *kvm, struct kvm_page_fault *fault
 	 * on RET_PF_SPURIOUS until the update completes, or an actual spurious
 	 * case might go down the slow path. Either case will resolve itself.
 	 */
-	if (kvm->arch.has_private_mem &&
+	if (kvm->has_private_mem &&
 	    fault->is_private != kvm_mem_is_private(kvm, fault->gfn))
 		return false;
 
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 160849690c4c..4a5a9bf2a045 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -4910,11 +4910,11 @@ static int svm_vm_init(struct kvm *kvm)
 
 	if (type != KVM_X86_DEFAULT_VM &&
 	    type != KVM_X86_SW_PROTECTED_VM) {
-		kvm->arch.has_protected_state =
+		kvm->has_protected_state =
 			(type == KVM_X86_SEV_ES_VM || type == KVM_X86_SNP_VM);
 		to_kvm_sev_info(kvm)->need_init = true;
 
-		kvm->arch.has_private_mem = (type == KVM_X86_SNP_VM);
+		kvm->has_private_mem = (type == KVM_X86_SNP_VM);
 	}
 
 	if (!pause_filter_count || !pause_filter_thresh)
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 2d507ec1d898..ade5835ca625 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5558,7 +5558,7 @@ static int kvm_vcpu_ioctl_x86_get_debugregs(struct kvm_vcpu *vcpu,
 	unsigned long val;
 	unsigned int i;
 
-	if (vcpu->kvm->arch.has_protected_state &&
+	if (vcpu->kvm->has_protected_state &&
 	    vcpu->arch.guest_state_protected)
 		return -EINVAL;
 
@@ -5579,7 +5579,7 @@ static int kvm_vcpu_ioctl_x86_set_debugregs(struct kvm_vcpu *vcpu,
 {
 	unsigned int i;
 
-	if (vcpu->kvm->arch.has_protected_state &&
+	if (vcpu->kvm->has_protected_state &&
 	    vcpu->arch.guest_state_protected)
 		return -EINVAL;
 
@@ -5622,7 +5622,7 @@ static int kvm_vcpu_ioctl_x86_get_xsave2(struct kvm_vcpu *vcpu,
 			     XFEATURE_MASK_FPSSE;
 
 	if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
-		return vcpu->kvm->arch.has_protected_state ? -EINVAL : 0;
+		return vcpu->kvm->has_protected_state ? -EINVAL : 0;
 
 	fpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu, state, size,
 				       supported_xcr0, vcpu->arch.pkru);
@@ -5640,7 +5640,7 @@ static int kvm_vcpu_ioctl_x86_set_xsave(struct kvm_vcpu *vcpu,
 					struct kvm_xsave *guest_xsave)
 {
 	if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
-		return vcpu->kvm->arch.has_protected_state ? -EINVAL : 0;
+		return vcpu->kvm->has_protected_state ? -EINVAL : 0;
 
 	return fpu_copy_uabi_to_guest_fpstate(&vcpu->arch.guest_fpu,
 					      guest_xsave->region,
@@ -5651,7 +5651,7 @@ static int kvm_vcpu_ioctl_x86_set_xsave(struct kvm_vcpu *vcpu,
 static int kvm_vcpu_ioctl_x86_get_xcrs(struct kvm_vcpu *vcpu,
 				       struct kvm_xcrs *guest_xcrs)
 {
-	if (vcpu->kvm->arch.has_protected_state &&
+	if (vcpu->kvm->has_protected_state &&
 	    vcpu->arch.guest_state_protected)
 		return -EINVAL;
 
@@ -5672,7 +5672,7 @@ static int kvm_vcpu_ioctl_x86_set_xcrs(struct kvm_vcpu *vcpu,
 {
 	int i, r = 0;
 
-	if (vcpu->kvm->arch.has_protected_state &&
+	if (vcpu->kvm->has_protected_state &&
 	    vcpu->arch.guest_state_protected)
 		return -EINVAL;
 
@@ -6286,7 +6286,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 #endif
 	case KVM_GET_SREGS2: {
 		r = -EINVAL;
-		if (vcpu->kvm->arch.has_protected_state &&
+		if (vcpu->kvm->has_protected_state &&
 		    vcpu->arch.guest_state_protected)
 			goto out;
 
@@ -6303,7 +6303,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	}
 	case KVM_SET_SREGS2: {
 		r = -EINVAL;
-		if (vcpu->kvm->arch.has_protected_state &&
+		if (vcpu->kvm->has_protected_state &&
 		    vcpu->arch.guest_state_protected)
 			goto out;
 
@@ -11552,7 +11552,7 @@ static void __get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 
 int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
-	if (vcpu->kvm->arch.has_protected_state &&
+	if (vcpu->kvm->has_protected_state &&
 	    vcpu->arch.guest_state_protected)
 		return -EINVAL;
 
@@ -11597,7 +11597,7 @@ static void __set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 
 int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
-	if (vcpu->kvm->arch.has_protected_state &&
+	if (vcpu->kvm->has_protected_state &&
 	    vcpu->arch.guest_state_protected)
 		return -EINVAL;
 
@@ -11673,7 +11673,7 @@ static void __get_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2)
 int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs)
 {
-	if (vcpu->kvm->arch.has_protected_state &&
+	if (vcpu->kvm->has_protected_state &&
 	    vcpu->arch.guest_state_protected)
 		return -EINVAL;
 
@@ -11944,7 +11944,7 @@ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 {
 	int ret;
 
-	if (vcpu->kvm->arch.has_protected_state &&
+	if (vcpu->kvm->has_protected_state &&
 	    vcpu->arch.guest_state_protected)
 		return -EINVAL;
 
@@ -12065,7 +12065,7 @@ int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 	struct fxregs_state *fxsave;
 
 	if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
-		return vcpu->kvm->arch.has_protected_state ? -EINVAL : 0;
+		return vcpu->kvm->has_protected_state ? -EINVAL : 0;
 
 	vcpu_load(vcpu);
 
@@ -12088,7 +12088,7 @@ int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 	struct fxregs_state *fxsave;
 
 	if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
-		return vcpu->kvm->arch.has_protected_state ? -EINVAL : 0;
+		return vcpu->kvm->has_protected_state ? -EINVAL : 0;
 
 	vcpu_load(vcpu);
 
@@ -12637,7 +12637,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		return -EINVAL;
 
 	kvm->arch.vm_type = type;
-	kvm->arch.has_private_mem =
+	kvm->has_private_mem =
 		(type == KVM_X86_SW_PROTECTED_VM);
 
 	ret = kvm_page_track_init(kvm);
diff --git a/include/linux/cc_platform.h b/include/linux/cc_platform.h
index 60693a145894..86acbcad590b 100644
--- a/include/linux/cc_platform.h
+++ b/include/linux/cc_platform.h
@@ -91,6 +91,7 @@ enum cc_attr {
 	 */
 	CC_ATTR_HOTPLUG_DISABLED,
 
+#ifndef __GENKSYMS__
 	/**
 	 * @CC_ATTR_HOST_SEV_SNP: AMD SNP enabled on the host.
 	 *
@@ -98,6 +99,7 @@ enum cc_attr {
 	 * enabled to run SEV-SNP guests.
 	 */
 	CC_ATTR_HOST_SEV_SNP,
+#endif
 };
 
 #ifdef CONFIG_ARCH_HAS_CC_PLATFORM
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 75836998662a..9554373d19fa 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -842,6 +842,11 @@ struct kvm {
 	struct xarray mem_attr_array;
 #endif
 	char stats_id[KVM_STATS_NAME_SIZE];
+
+#ifndef __GENKSYMS__
+	bool has_private_mem;
+	bool has_protected_state;
+#endif
 };
 
 #define kvm_err(fmt, ...) \
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index f94b4a7dfd87..019d4ea0c10d 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -553,8 +553,10 @@ struct kvm_run {
 			__u64 gpa;
 			__u64 size;
 		} memory_fault;
+#ifndef __GENKSYMS__
 		/* KVM_EXIT_VMGEXIT */
 		struct kvm_user_vmgexit vmgexit;
+#endif
 		/* Fix the size of the union. */
 		char padding[256];
 	};
-- 
2.45.1

