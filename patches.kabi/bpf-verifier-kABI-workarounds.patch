From: Shung-Hsi Yu <shung-hsi.yu@suse.com>
Subject: kABI: bpf: verifier kABI workaround
Patch-mainline: never, kabi
References: bsc#1225903

Workaround kABI breakage in bpf_func_state, bpf_verifier_state, and bpf_insn_aux_data:

- Commit bb124da69c47d ("bpf: keep track of max number of bpf_loop callback
  iterations") added a callback_depth field in "struct bpf_func_state" and
  break kABI, workaround the breakage by moving the callback_depth field to the
  end of the structure as it is only accessed through pointers.

  However, copy_func_state() in kernel/bpf/verifier.c has to be updated as well
  so it will be copied along with other fields before the acquired_refs field.

- Commit ab5cfac139ab8 ("bpf: verify callbacks as if they are called unknown
  number of times") and commit 2a0992829ea38 ("bpf: correct loop detection for
  iterators convergence") added several fields in struct bpf_verifier_state.
  Move the new fields to the end of the structure and hide them to workaround
  kABI breakage, with the exception of "bool used_as_loop_entry", which fit
  into the padding and doesn't need to be move.

  While struct bpf_verifer_state is embedded in struct bpf_verfier_state_list,
  it is only allocated in is_state_visited(), and thus the change of size will
  not cause breakage.

- Commit ab5cfac139ab8 ("bpf: verify callbacks as if they are called unknown
  number of times") adds a calls_callback field to struct bpf_insn_aux_data as
  the last field. Simply hiding it from kernel symbol generation is enough
  since it is already the last field and struct bpf_insn_aux_data is accessed
  through pointers.

- Commit 45b5623f2d72 ("bpf: rearrange bpf_func_state fields to save a bit of
  memory") rearranged field in bpf_func_state, simply move the back to where
  they were.

---
 include/linux/bpf_verifier.h |   76 ++++++++++++++++++++++++++++++-------------
 kernel/bpf/verifier.c        |    1 
 2 files changed, 55 insertions(+), 22 deletions(-)

--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -302,24 +302,17 @@ struct bpf_func_state {
 	 * void foo(void) { bpf_timer_set_callback(,foo); }
 	 */
 	u32 async_entry_cnt;
-	struct bpf_retval_range callback_ret_range;
 	bool in_callback_fn;
+	struct bpf_retval_range callback_ret_range;
 	bool in_async_callback_fn;
-	/* For callback calling functions that limit number of possible
-	 * callback executions (e.g. bpf_loop) keeps track of current
-	 * simulated iteration number.
-	 * Value in frame N refers to number of times callback with frame
-	 * N+1 was simulated, e.g. for the following call:
-	 *
-	 *   bpf_loop(..., fn, ...); | suppose current frame is N
-	 *                           | fn would be simulated in frame N+1
-	 *                           | number of simulations is tracked in frame N
-	 */
-	u32 callback_depth;
 
 	/* The following fields should be last. See copy_func_state() */
 	int acquired_refs;
 	struct bpf_reference_state *refs;
+	/* Size of the current stack, in bytes. The stack state is tracked below, in
+	 * `stack`. allocated_stack is always a multiple of BPF_REG_SIZE.
+	 */
+	int allocated_stack;
 	/* The state of the stack. Each element of the array describes BPF_REG_SIZE
 	 * (i.e. 8) bytes worth of stack memory.
 	 * stack[0] represents bytes [*(r10-8)..*(r10-1)]
@@ -328,10 +321,24 @@ struct bpf_func_state {
 	 * stack[allocated_stack/8 - 1] represents [*(r10-allocated_stack)..*(r10-allocated_stack+7)]
 	 */
 	struct bpf_stack_state *stack;
-	/* Size of the current stack, in bytes. The stack state is tracked below, in
-	 * `stack`. allocated_stack is always a multiple of BPF_REG_SIZE.
+
+	/* syu: fields that after this has to be copied manully in
+	 * copy_func_state() to keep the code working while preserving kABI at
+	 * the same time.
 	 */
-	int allocated_stack;
+#ifndef __GENKSYMS__
+	/* For callback calling functions that limit number of possible
+	 * callback executions (e.g. bpf_loop) keeps track of current
+	 * simulated iteration number.
+	 * Value in frame N refers to number of times callback with frame
+	 * N+1 was simulated, e.g. for the following call:
+	 *
+	 *   bpf_loop(..., fn, ...); | suppose current frame is N
+	 *                           | fn would be simulated in frame N+1
+	 *                           | number of simulations is tracked in frame N
+	 */
+	u32 callback_depth;
+#endif /* __GENKSYMS__ */
 };
 
 struct bpf_idx_pair {
@@ -398,15 +405,27 @@ struct bpf_verifier_state {
 	struct bpf_active_lock active_lock;
 	bool speculative;
 	bool active_rcu_lock;
+#ifndef __GENKSYMS__
 	/* If this state was ever pointed-to by other state's loop_entry field
 	 * this flag would be set to true. Used to avoid freeing such states
 	 * while they are still in use.
 	 */
 	bool used_as_loop_entry;
+#endif /* __GENKSYMS__ */
 
 	/* first and last insn idx of this verifier state */
 	u32 first_insn_idx;
 	u32 last_insn_idx;
+	/* jmp history recorded from first to last.
+	 * backtracking is using it to go from last to first.
+	 * For most states jmp_history_cnt is [0-3].
+	 * For loops can go up to ~40.
+	 */
+	struct bpf_idx_pair *jmp_history;
+	u32 jmp_history_cnt;
+#ifndef __GENKSYMS__
+	u32 dfs_depth;
+	u32 callback_unroll_depth;
 	/* If this state is a part of states loop this field points to some
 	 * parent of this state such that:
 	 * - it is also a member of the same states loop;
@@ -417,17 +436,28 @@ struct bpf_verifier_state {
 	 * See get_loop_entry() for more information.
 	 */
 	struct bpf_verifier_state *loop_entry;
-	/* jmp history recorded from first to last.
-	 * backtracking is using it to go from last to first.
-	 * For most states jmp_history_cnt is [0-3].
-	 * For loops can go up to ~40.
-	 */
+#endif /* __GENKSYMS__ */
+};
+
+struct bpf_verifier_state_old {
+	struct bpf_func_state *frame[MAX_CALL_FRAMES];
+	struct bpf_verifier_state *parent;
+	u32 branches;
+	u32 insn_idx;
+	u32 curframe;
+	struct bpf_active_lock active_lock;
+	bool speculative;
+	bool active_rcu_lock;
+	u32 first_insn_idx;
+	u32 last_insn_idx;
 	struct bpf_idx_pair *jmp_history;
 	u32 jmp_history_cnt;
-	u32 dfs_depth;
-	u32 callback_unroll_depth;
 };
 
+/* Make sure "bool used_as_loop_entry" field does not affect memory layout */
+static_assert(offsetof(struct bpf_verifier_state, first_insn_idx) ==
+		offsetof(struct bpf_verifier_state_old, first_insn_idx));
+
 #define bpf_get_spilled_reg(slot, frame)				\
 	(((slot < frame->allocated_stack / BPF_REG_SIZE) &&		\
 	  (frame->stack[slot].slot_type[0] == STACK_SPILL))		\
@@ -533,10 +563,12 @@ struct bpf_insn_aux_data {
 	 * this instruction, regardless of any heuristics
 	 */
 	bool force_checkpoint;
+#ifndef __GENKSYMS__
 	/* true if instruction is a call to a helper function that
 	 * accepts callback function as a parameter.
 	 */
 	bool calls_callback;
+#endif /* __GENKSYMS__ */
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -1753,6 +1753,7 @@ static int copy_func_state(struct bpf_fu
 	int err;
 
 	memcpy(dst, src, offsetof(struct bpf_func_state, acquired_refs));
+	dst->callback_depth = src->callback_depth;
 	err = copy_reference_state(dst, src);
 	if (err)
 		return err;
