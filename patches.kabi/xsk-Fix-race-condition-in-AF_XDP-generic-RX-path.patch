From: Davide Benini <davide.benini@suse.com>
Date: Tue Jul  1 18:32:39 CEST 2025
Subject: kABI workaround for xsk: Fix race condition in AF_XDP generic RX path
Patch-mainline: Never, kABI workaround
References: CVE-2025-37920 bsc#1243479

Commit a1356ac7749cafc4e27aa62c0c4604b5dca4983e moves rx_lock from xsk_socket
to xsk_buff_pool.

struct xsk_buff_pool initiliazer is xp_create_and_assign_umem(); so it seems
safe adding a field to the structure in the right position (using holes), as
the structure is initialzed in a specific place.

Keep the space in xdp_sock occupied by the no more used spinlock_t


Signed-off-by: Davide Benini <davide.benini@suse.com>
Signed-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>
---
 include/net/xdp_sock.h      |    8 ++++-
 include/net/xsk_buff_pool.h |   63 ++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 70 insertions(+), 1 deletion(-)

--- a/include/net/xdp_sock.h
+++ b/include/net/xdp_sock.h
@@ -71,7 +71,13 @@ struct xdp_sock {
 	 */
 	u32 tx_budget_spent;
 
-	/* Statistics */
+#ifdef __GENKSYMS__
+	/* Protects generic receive. */
+	spinlock_t rx_lock;
+#else
+	spinlock_t rx_lock_unused __attribute__((deprecated));
+#endif
+
 	u64 rx_dropped;
 	u64 rx_queue_full;
 
--- a/include/net/xsk_buff_pool.h
+++ b/include/net/xsk_buff_pool.h
@@ -9,6 +9,8 @@
 #include <linux/dma-mapping.h>
 #include <linux/bpf.h>
 #include <net/xdp.h>
+#include <linux/build_bug.h>  /* for static_assert() */
+#include <linux/stddef.h>     /* for offsetof() */
 
 struct xsk_buff_pool;
 struct xdp_rxq_info;
@@ -56,8 +58,58 @@ struct xsk_buff_pool {
 	refcount_t users;
 	struct xdp_umem *umem;
 	struct work_struct work;
+	struct list_head free_list;
+	struct list_head xskb_list;
+	u32 heads_cnt;
+	u16 queue_id;
+#ifndef __GENKSYMS__
 	/* Protects generic receive in shared and non-shared umem mode. */
 	spinlock_t rx_lock;
+#endif
+
+	/* Data path members as close to free_heads at the end as possible. */
+	struct xsk_queue *fq ____cacheline_aligned_in_smp;
+	struct xsk_queue *cq;
+	/* For performance reasons, each buff pool has its own array of dma_pages
+	 * even when they are identical.
+	 */
+	dma_addr_t *dma_pages;
+	struct xdp_buff_xsk *heads;
+	struct xdp_desc *tx_descs;
+	u64 chunk_mask;
+	u64 addrs_cnt;
+	u32 free_list_cnt;
+	u32 dma_pages_cnt;
+	u32 free_heads_cnt;
+	u32 headroom;
+	u32 chunk_size;
+	u32 chunk_shift;
+	u32 frame_len;
+	u8 tx_metadata_len; /* inherited from umem */
+	u8 cached_need_wakeup;
+	bool uses_need_wakeup;
+	bool dma_need_sync;
+	bool unaligned;
+	bool tx_sw_csum;
+	void *addrs;
+	/* Mutual exclusion of the completion ring in the SKB mode. Two cases to protect:
+	 * NAPI TX thread and sendmsg error paths in the SKB destructor callback and when
+	 * sockets share a single cq when the same netdev and queue id is shared.
+	 */
+	spinlock_t cq_lock;
+	struct xdp_buff_xsk *free_heads[];
+};
+
+struct __orig_xsk_buff_pool {
+	/* Members only used in the control path first. */
+	struct device *dev;
+	struct net_device *netdev;
+	struct list_head xsk_tx_list;
+	/* Protects modifications to the xsk_tx_list */
+	spinlock_t xsk_tx_list_lock;
+	refcount_t users;
+	struct xdp_umem *umem;
+	struct work_struct work;
 	struct list_head free_list;
 	struct list_head xskb_list;
 	u32 heads_cnt;
@@ -95,6 +147,17 @@ struct xsk_buff_pool {
 	spinlock_t cq_lock;
 	struct xdp_buff_xsk *free_heads[];
 };
+/* shung-hsi.yu: the following kABI check has been problematic for other that
+ * build kernel with their own configuration that is different from our SUSE
+ * default ones. Prevent these checks temporarily until we determined a
+ * solution to signal kABI verification.
+ *
+ *	static_assert(offsetof(struct xsk_buff_pool, queue_id) ==
+ *		      offsetof(struct __orig_xsk_buff_pool, queue_id));
+ *	static_assert(offsetof(struct xsk_buff_pool, fq) ==
+ *		      offsetof(struct __orig_xsk_buff_pool, fq));
+ */
+
 
 /* Masks for xdp_umem_page flags.
  * The low 12-bits of the addr will be 0 since this is the page address, so we
