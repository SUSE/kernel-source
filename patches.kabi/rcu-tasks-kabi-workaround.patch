From: Petr Pavlu <ppavlu@suse.cz>
Subject: kABI workaround for RCU Tasks
Patch-mainline: Never, kABI workaround
References: bsc#1246298

The backport of:

	bfe93930ea1e ("rcu-tasks: Add data to eliminate RCU-tasks/do_exit() deadlocks")

added two new fields to the task struct. Put them to an appropriate
task struct gap.

Signed-off-by: Petr Pavlu <ppavlu@suse.cz>
Signed-off-by: Frederic Weisbecker <fweisbecker@suse.de>
---
 include/linux/sched.h |   98 ++++++++++++++++++++++++++++++++++++++++++++++++--
 1 file changed, 96 insertions(+), 2 deletions(-)

--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -813,6 +813,11 @@ struct task_struct {
 	struct uclamp_se		uclamp[UCLAMP_CNT];
 #endif
 
+#ifndef __GENKSYMS__
+	int				rcu_tasks_exit_cpu;
+	struct list_head		rcu_tasks_exit_list;
+#endif
+
 	struct sched_statistics         stats;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
@@ -848,8 +853,6 @@ struct task_struct {
 	u8				rcu_tasks_idx;
 	int				rcu_tasks_idle_cpu;
 	struct list_head		rcu_tasks_holdout_list;
-	int				rcu_tasks_exit_cpu;
-	struct list_head		rcu_tasks_exit_list;
 #endif /* #ifdef CONFIG_TASKS_RCU */
 
 #ifdef CONFIG_TASKS_TRACE_RCU
@@ -1557,6 +1560,97 @@ struct task_struct {
 	 */
 };
 
+struct __orig_task_struct {
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+	/*
+	 * For reasons of header soup (see current_thread_info()), this
+	 * must be the first element of task_struct.
+	 */
+	struct thread_info		thread_info;
+#endif
+	unsigned int			__state;
+
+	/* saved state for "spinlock sleepers" */
+	unsigned int			saved_state;
+
+	/*
+	 * This begins the randomizable portion of task_struct. Only
+	 * scheduling-critical items should be added above here.
+	 */
+	randomized_struct_fields_start
+
+	void				*stack;
+	refcount_t			usage;
+	/* Per task flags (PF_*), defined further below: */
+	unsigned int			flags;
+	unsigned int			ptrace;
+
+#ifdef CONFIG_SMP
+	int				on_cpu;
+	struct __call_single_node	wake_entry;
+	unsigned int			wakee_flips;
+	unsigned long			wakee_flip_decay_ts;
+	struct task_struct		*last_wakee;
+
+	/*
+	 * recent_used_cpu is initially set as the last CPU used by a task
+	 * that wakes affine another task. Waker/wakee relationships can
+	 * push tasks around a CPU where each wakeup moves to the next one.
+	 * Tracking a recently used CPU allows a quick search for a recently
+	 * used CPU that may be idle.
+	 */
+	int				recent_used_cpu;
+	int				wake_cpu;
+#endif
+	int				on_rq;
+
+	int				prio;
+	int				static_prio;
+	int				normal_prio;
+	unsigned int			rt_priority;
+
+	struct sched_entity		se;
+	struct sched_rt_entity		rt;
+	struct sched_dl_entity		dl;
+	const struct sched_class	*sched_class;
+
+#ifdef CONFIG_SCHED_CORE
+	struct rb_node			core_node;
+	unsigned long			core_cookie;
+	unsigned int			core_occupation;
+#endif
+
+#ifdef CONFIG_CGROUP_SCHED
+	struct task_group		*sched_task_group;
+#endif
+
+#ifdef CONFIG_UCLAMP_TASK
+	/*
+	 * Clamp values requested for a scheduling entity.
+	 * Must be updated with task_rq_lock() held.
+	 */
+	struct uclamp_se		uclamp_req[UCLAMP_CNT];
+	/*
+	 * Effective clamp values used for a scheduling entity.
+	 * Must be updated with task_rq_lock() held.
+	 */
+	struct uclamp_se		uclamp[UCLAMP_CNT];
+#endif
+
+	struct sched_statistics         stats;
+
+	/* [...] */
+
+	randomized_struct_fields_end
+};
+
+#ifdef CONFIG_CGROUP_SCHED
+suse_kabi_static_assert(offsetof(struct task_struct, sched_task_group) ==
+			offsetof(struct __orig_task_struct, sched_task_group));
+#endif
+suse_kabi_static_assert(offsetof(struct task_struct, stats) ==
+			offsetof(struct __orig_task_struct, stats));
+
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->thread_pid;
